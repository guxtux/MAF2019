\input{../preambulo_doc}
\title{Funciones de Bessel \\ {\large Tema 5 - Matemáticas Avanzadas de la Física}\vspace{-1.5\baselineskip}}
\date{ }
\author{}
\begin{document}
\maketitle
\fontsize{14}{14}\selectfont
%Sadri Hassani - Mathematical methods for students of physics. Chap. 27 Laplace equation: cylindrical coordinates
Antes de desarrollar un problema con una geometría cilíndrica, consideremos una pregunta que tiene implicaciones más generales. Vimos en el Tema 2 que la separación de variables condujo a un sistema de EDO en las que aparecían ciertas constantes de separación, y que elegir el signo de esa constante nos lleva a una forma funcional diferente de la solución general.
\par
Por ejemplo, para una ecuación como
\begin{align*}
\dv[2]{x}{t} - k \, x = 0
\end{align*}
se pueden tener soluciones exponenciales si $k > 0$ y soluciones trigonométricas si $k < 0$ Uno no puede asignar a priori un signo específico a $k$. Por lo tanto, la forma general de la solución es indeterminada. Sin embargo, \emph{una vez que se imponen las CDF}, las soluciones únicas surgirán independientemente de la forma funcional inicial de las soluciones.
\par
El siguiente argumento ilustra este punto en la ED angular resultante de la separación de variables para la ecuación de Laplace en coordenadas cilíndricas.
\section{Funciones de Bessel.}
La separación de variables en coordenadas cilíndricas para la ecuación de Laplace
\begin{align*}
\laplacian{\Phi} = 0
\end{align*}
mediante la solución propuesta $\Phi (\rho, \varphi, z) = R(\rho) \, S(\varphi) \, Z(z)$, nos lleva a un sistema de tres EDO:
\begin{align}
\begin{aligned}
\dv{\rho} \left( \rho \dv{R}{\rho} \right) + \left( \lambda \, \rho + \dfrac{\mu}{\rho} \right) \, R &= 0 \\
\dv[2]{S}{\varphi} - \mu \, S &= 0 \\
\dv[2]{Z}{z} - \lambda \, Z &= 0
\end{aligned}
\label{eq:ecuacion_27_01}
\end{align}
Si nos fijamos en la segunda ecuación cuya solución más general podemos escribir como
\begin{align}
S (\varphi) = \begin{cases}
A \, \exp(\sqrt{\mu} \, \varphi) + B \, \exp(-\sqrt{\mu} \, \varphi) & \mbox{ si } \mu \neq 0 \\
C \,\varphi + D & \mbox { si } \mu = 0
\end{cases}
\label{eq:ecuacion_27_02}
\end{align}
No importa que tipo de CDF se impongan al potencial $\Phi$, ya que debe de devolver el mismo valor en $\varphi$ y en $\varphi + 2 \pi$, mientras que semantengan las otras dos variables fijas\footnote{Este argumento es válido solo para los casos físicos definidos para todo el rango de $\varphi$. Si la región de interés restringe a $\varphi$ en un subconjunto del intervalo $[0, 2 \pi]$, el argumento ya no funciona.}. Esto es debido a que $(\rho, \varphi, z)$ y $(\rho, \varphi + 2 \pi, z)$ representan físicamente al mismo punto en el espacio. Se sigue entonces que
\begin{align*}
R(\rho) \, S(\varphi) \, Z(z) = R(\rho)  \, S(\varphi + 2 \pi) , Z(z) \Longrightarrow S(\varphi + 2 \pi) = S (\varphi)
\end{align*}
ya que la identidad se mantiene para todos los valores de $\rho$ y $z$. Si la última relación es verdadera para el caso de $\mu = 0$, entonces tenemos que $C = 0$ y $S(\varphi) = D$. Para $\mu \neq 0$, la ec. (\ref{eq:ecuacion_27_02}) es
\begin{align*}
A \exp(\sqrt{\mu}(\varphi + 2 \pi)) + B \exp(- \sqrt{\mu}(\varphi + 2 \pi)) = A \exp(\sqrt{\mu} \varphi) + B \exp(-\sqrt{\mu} \varphi)
\end{align*}
o también
\begin{align*}
A \, \exp(\sqrt{\mu} \, \varphi) \, (\exp(\sqrt{\mu} \, 2 \pi) - 1) + B \, \exp(- \sqrt{\mu} \, \varphi) \, (\exp(- \sqrt{\mu} \, 2 \pi) - 1) = 0
\end{align*}
que debe de cumplirse para todo $\varphi$; la única manera en la que esto puede ocurrir (cuidando de que $A$ y $B$ no sean nulos) es mediante
\begin{align*}
\exp(\sqrt{\mu} \, 2 \pi) - 1 = 0 \hspace{1cm} y \hspace{1cm} \exp(- \sqrt{\mu} \, 2 \pi) - 1 = 0
\end{align*}
En ambos casos se tiene que $\exp(\sqrt{\mu} \, 2 \, \pi) = 1$
\par
Si nos limitamos a valores reales de $\mu$, obtendremos soluciones triviales. Para prevenir esto, tenemos que hacer
\begin{align*}
\sqrt{\mu} = i \, m \hspace{1cm} m = 0, \pm 1, \pm 2, \ldots
\end{align*}
o
\begin{align*}
\mu = -m^{2}  \hspace{1cm} m = 0, \pm 1, \pm 2, \ldots
\end{align*}
Con esta elección de $\mu$, la ED para $S(\varphi)$ es
\begin{align*}
S^{\prime \prime} + m^{2} \, S = 0
\end{align*}
que tiene como solución general: una suma de funciones trigonométricas.
\begin{teo}
Para todos los problemas físicos para los cuales el ángulo azimutal varíe entre $0$ y $2 \pi$, uno está forzado a restringir el valor de $\mu$ al negativo de la raíz cuadrada de un entero. La solución para la parte angular es entonces
\begin{equation}
S (\varphi) = A_{m} \, \cos m \varphi + B_{m} \, \sin m \varphi, \hspace{1cm} m = 0, 1, 2, \ldots
\label{eq:ecuacion_27_03}
\end{equation}
donde $A_{m}$ y $B_{m}$ son constantes que pueden ser distintas para diferentes valores de $m$.
\end{teo}
Los valores negativos de $m$ no darán lugar a nuevas soluciones, por lo que no se incluyen en el rango de $m$. El caso de $\mu = 0$ no se necesita tratar por separado, ya que la solución aceptable para este caso es $S = D = \mbox{ constante}$, la cual es la que se obtiene en la ec. (\ref{eq:ecuacion_27_03}) cuando $m = 0$.
\par
La ED para $Z(z)$ es independiente de $m$ y tiene una solución exponencial si $\lambda > 0$ y una solución trigonométrica si $\lambda < 0$. Asumiendo ésta forma y escribiendo $\lambda \equiv l^{2}$, se tiene
\begin{equation}
Z(z) = A \, e^{l \, z} + B \, e^{- l \, z}
\label{eq:ecuacion_27_04}
\end{equation}
La más familiar de las ED es la ecuación radial, en términos de $l = \sqrt{\lambda}$, se puede escribir como
\begin{equation}
\dv[2]{R}{\rho} + \dfrac{1}{\rho} \, \dv{R}{\rho} + \left( l^{2} - \dfrac{m^{2}}{\rho^{2}} \right) \, R = 0
\label{eq:ecuacion_27_05}
\end{equation}
Además, si definimos la variable $v = l\, \rho$, podemos transformar la ec. (\ref{eq:ecuacion_27_05}) en la forma:
\begin{equation}
\dv[2]{R}{\rho} + \dfrac{1}{v} \, \dv{R}{v} + \left( 1 - \dfrac{m^{2}}{v^{2}} \right) \, R = 0
\label{eq:ecuacion_27_06}
\end{equation}
Tanto la ec. (\ref{eq:ecuacion_27_05}) o (\ref{eq:ecuacion_27_06}), es una de las ED de la física matemática más famosas: \textbf{la ecuación diferencial de Bessel}.
\section{Soluciones para la ED de Bessel.}
El método de Frobenius es una manera efectiva para encontrar las soluciones de las EDO. Al reescribir la ec. (\ref{eq:ecuacion_27_06}) multiplicando por $v^{2}$ para convertir todos sus coeficientes en polinomios como lo sugiere la ecuación (\ref{eq:ecuacion_26_07}).
\begin{equation}
p_{2}(x) \, \dv[2]{y}{x} + p_{1}(x) \, \dv{y}{x} + p_{0} (x) \, y
 = 0
\label{eq:ecuacion_26_07}
\end{equation}
Esto nos lleva a
\begin{equation}
v^{2} \, \dv[2]{R}{v} + v \dv{R}{v} + (v^{2} - m^{2}) \, R = 0
\label{eq:ecuacion_27_07}
\end{equation}
Como $v^{2}$ se anula cuando $v = 0$, podemos proponer una solución de la forma
\begin{align*}
R(v) &= v^{s} \sum_{k=0}^{\infty} c_{k} \, v^{k} = \\
&= \sum_{k=0}^{\infty} c_{k} \, v^{k+s}
\end{align*}
de la cual obtenemos al diferenciar en dos ocasiones:
\begin{align*}
v \, \dv{R}{v} &= \sum_{k=0}^{\infty} c_{k} (k + s) \, v^{k+s} \\
v^{2} \, \dv[2]{R}{v} &= \sum_{k=0}^{\infty} c_{k} (k + s)(k + s - 1) \, v^{k+s} 
\end{align*}
sustituyendo éstos, como
\begin{align*}
(v^{2} - m^{2}) \sum_{k=0}^{\infty} c_{k} \, v^{k+s}
\end{align*}
en la ED, nos conduce a
\begin{align*}
\sum_{k=0}^{\infty} c_{k} [\underbrace{k + s + (k + s)(k + s - 1)}_{=(k+s)^{2}} - m^{2} ] \, v^{k+s} + \sum_{k=0}^{\infty} c_{k} \, v^{k+s+2} = 0
\end{align*}
Para encontrar la relación de recurrencia, necesitamos que el valor de la potencia de $v$ sea el mismo en las sumas, por lo que re-escribimos la primera suma como
\begin{align*}
c_{0}(s^{2} &- m^{2}) v^{s} + c_{1} [(s + 1)^{2}- m^{2}] v^{s+1} + \sum_{k=2}^{\infty} c_{k} [(k + s)^{2} - m^{2}] v^{k+s} \\
&= c_{0}(s^{2} - m^{2}) v^{s} + c_{1} [(s + 1)^{2}- m^{2}] v^{s+1} \\
&+ \sum_{n=0}^{\infty} c_{n+2} [(n + 2 + s)^{2} - m^{2}] v^{n+2+s}
\end{align*}
donde en la segunda línea, usamos $n = k -2$. Como el índice $n$ es mudo, lo podemos regresar de nuevo al índice $k$. Entonces, seguimos con que
\begin{align*}
c_{0}(s^{2} &- m^{2}) v^{s} + c_{1} [(s + 1)^{2}- m^{2}] v^{s+1} + \\
&+ \sum_{k=0}^{\infty} \left\{ c_{k+2} [(k + 2 + s)^{2} - m^{2}] + c_{k} \right\} \, v^{k+2+s} = 0
\end{align*}
Suponemos que $c_{0} \neq 0$ y que todos los demás coeficientes de las potencias de $v$ se anulan, obtenemos
\begin{align*}
s^{2} &= m^{2} \\
c_{1} [(s + 1)^{2}- m^{2}] &= 0\\
c_{k+2} [(k + 2 + s)^{2} - m^{2}] + c_{k} &= 0
\end{align*}
La primera ecuación nos dice que $m = \pm s$. Por lo que al usarlo en la segunda ecuación, resulta en
\begin{align*}
c_{1} (2 \, s + 1) = 0 \hspace{0.5cm} \Rightarrow \hspace{0.5cm} c_{1} = 0 \hspace{1cm} \mbox{ o } \hspace{0.5cm} s = - \dfrac{1}{2}
\end{align*}
La elección $s = -1/2$ nos devuelve que $m = \mp 1/2$ la cual no es aceptable\footnote{En realidad, los problemas que surgen de otras áreas de la física más allá de la electrostática y la transferencia de calor en estado estacionario permiten valores de $m$ no enteros. Sin embargo, no trataremos tales problemas aquí.}, por lo que decidimos que $m$ sea un entero positivo. Por lo tanto concluimos que $s = \pm m$ y $c_{1} = 0$. 
\par
Se sigue que la regla de recurrencia para todos los coeficientes impares son cero. La serie de Frobenius entonces será
\begin{equation}
R(v) = v^{\pm m} \sum_{k=0}^{\infty} c_{2k} \, v^{2k}, \hspace{1.5cm} \dfrac{c_{2k+2}}{c_{2k}} = - \dfrac{1}{(2k + 2 + s)^{2} - m^{2}}
\label{eq:ecuacion_27_08}
\end{equation}
La prueba del cociente para la convergencia de la serie nos lleva a
\begin{align*}
\lim_{k \to \infty} \abs{\dfrac{c_{2k+2} \, v^{2k+2}}{c_{2k} \, v^{2k}}} = \lim_{k \to \infty} \abs{\dfrac{1}{(2k + 2 + s)^{2} -m^{2}}} \, v^{2} = 0
\end{align*}
Lo que nos indica que la serie de la ec. (\ref{eq:ecuacion_27_08}) es convergente para todos los valores de $v$.
\par
Ahora usaremos la relación de recurrencia para obtener los coeficientes de la expansión. Reescribimos la relación de recuerrencia como
\begin{align*}
c_{k+2} = - \dfrac{1}{(k + 2 + s)^{2} - s^{2}} \, c_{k} = \dfrac{1}{(k + 2)(2 s + k + 2)} \, c_{k}
\end{align*}
donde hemos sustituido $s^{2}$ por $m^{2}$. Lo que nos devuelve
\begin{align*}
c_{2} &= - \dfrac{1}{2 (2 s + 2)} c_{0} \\
c_{4} &= - \dfrac{1}{4 (2 s + 4)} c_{2} = (-1)^{2} \dfrac{1}{4 (2 s + 4)} \, \dfrac{1}{2 (2 s + 2)} \, c_{0} \\
c_{6} &= - \dfrac{1}{6 (2 s + 6)} c_{4} = (-1)^{3} \dfrac{1}{6 (2 s + 6)} \, \dfrac{1}{4 (2 s + 4)} \, \dfrac{1}{2 (2 s + 2)} \, c_{0}
\end{align*}
y de manera general
\begin{align*}
c_{2k} = (-1)^{k} \dfrac{1}{\underbrace{2 k \cdot (2 k - 2) \ldots 2}_{=2^{k} \, k!} \underbrace{(2 s + 2 k)[2 s + (2 k - 2)] \ldots (2 s + 2)}_{=2^{k} (s + k)(s + k + 1) \ldots (s + 1)}} \, c_{0}
\end{align*}
Multiplicando el numerador y el denominador por $s!$, llegamos a
\begin{equation}
c_{2k} = (-1)^{k} \dfrac{s!}{2^{2k} \, k! \, (s+k)!} \, c_{0}
\label{eq:ecuacion_27_09}
\end{equation}
Sustituyendo la ec. (\ref{eq:ecuacion_27_09}) en la (\ref{eq:ecuacion_27_08}), nos conduce a
\begin{align*}
R(v) &= c_{0} \, s! \, v^{s} \sum_{k=0}^{\infty} \dfrac{(-1)^{k}}{2^{2k} \, k! \, (s + k)!} \, v^{2k} \\
&= c_{0} \, s! \, 2^{s} \, \left( \dfrac{v}{2} \right)^{s} \sum_{k=0}^{\infty} \dfrac{(-1)^{k}}{k! \, (s + k)!} \, \left( \dfrac{v}{2} \right)^{2 k} 
\end{align*}
donde sustituimos $s$ por $\pm m$ en el exponente de $v$ fuera de la suma. También absorbimos las potencias de $2$ en el denominador de la suma en las potencias de $v$, y fuera de la suma, multiplicamos y dividimos por $2^{s}$. Es habitual elegir que la constante arbitraria $c_{0}$ sea igual a $1/(s! \, 2^{s})$. Esto nos lleva a 
\begin{tcolorbox}
Las \textbf{funciones de Bessel} de orden $s$, que se escribe como $J_{s}$ y está dada por la serie
\begin{equation}
J_{s}(x) = \left( \dfrac{x}{2} \right)^{s} \sum_{k=0}^{\infty} \dfrac{(-1)^{k}}{k!\, (s + k)!} \, \left( \dfrac{x}{2} \right)^{2k}
\label{eq:ecuacion_27_10}
\end{equation}
la cual es convergente para todos los valores de $x$.
\end{tcolorbox}
Aunque la Ecuación (\ref{eq:ecuacion_27_10}) se obtuvo asumiendo que $m$, y por lo tanto $s$, era un número entero, al levantar esta restricción se obtendría una serie que es convergente en todas partes, y uno puede definir funciones de Bessel cuyas órdenes son números reales o incluso complejos. La única dificultad es interpretar correctamente $(s + n)!$ para $s$ no enteros. Pero esto es precisamente para lo que se inventó la función gamma. Por lo tanto, dejaremos que la ecuación (\ref{eq:ecuacion_27_10}) represente las funciones de Bessel de todos los órdenes.

\section{Segunda solución a la ED de Bessel.}
Se puede obtener una segunda solución de la ED de Bessel, usando la ec. (\ref{eq:ecuacion_24_06}), donde $p(x) = 1/x$.
\begin{equation}
f_{2}(x) =  f_{1}(x) \left\{ C + K \int_{\alpha}^{x} \dfrac{1}{f_{1}^{2} (s)} \, \exp( - \int_{c}^{s} p(t) \dd{t}) \dd{s} \right\}
\label{eq:ecuacion_24_06}
\end{equation}
Usando $J_{m}(x)$ como entrada, podemos generar la segunda solución; con $C = 0$ en la ec. (\ref{eq:ecuacion_24_06}), obtenemos
\begin{align*}
Z_{m}(x) = K \, J_{m}(x) \, \int_{\alpha}^{x} \dfrac{1}{J_{m}^{2}(x)} \exp(- \int_{c}^{u} \dfrac{\dd{t}}{t}) \dd{u} = A_{m} \, J_{m}(x) \int_{\alpha}^{x} \dfrac{\dd{u}}{u \, J_{m}^{2}(u)}
\end{align*}
donde $A_{m} \equiv K , c$ y $\alpha$ son constantes arbitrarias que se establecen por convención.
\par
Nótese que, contrario a $J_{m}(x)$, la solución $Z_{m}(x)$ no se comporta bien en $x = 0$, debido a la presencia de $u$ en el denominador del integrando.
\par
Aunque el procedimiento anterior genera una segunda solución para la ED  de Bessel, no es el procedimiento habitual. Resulta que para los no enteros $s$, la función de Bessel $J_{-s} (x)$ es independiente de $J_{s} (x)$ y se puede usar como una segunda solución. Sin embargo, una segunda solución más común es la combinación lineal
\begin{equation}
Y_{s} (x) = \dfrac{J_{s}(x) , \cos s \pi - J_{-s}(x)}{\sin s\pi}
\label{eq:ecuacion_27_11}
\end{equation}
que es llamada la función de Bessel de segunda clase, o \textbf{Funciones de Neumann}.
\par
Para enteros $s$ la función es indeterminada debido a que
\begin{align}
\begin{aligned}
J_{-m}(x) &= \left(\dfrac{x}{2} \right)^{-m} \sum_{n=0}^{\infty} \dfrac{(-1)^{m+n}}{(m+n)! \, \Gamma(n+1)} \, \left( \dfrac{x}{2} \right)^{2m +2n} = \\
&= (-1)^{m} \left( \dfrac{x}{2} \right)^{m} \sum_{n=0}^{\infty} \dfrac{(-1)^{n}}{\Gamma(m+n+1) \, n!} \, \left( \dfrac{x}{2} \right)^{2n} = \\
&= (-1)^{m} \, J_{m}(x)    
\end{aligned}
\label{eq:ecuacion_11_32}
\end{align}
y por la identidad $\cos n \pi = (-1)^{n}$. Por lo tanto, usamos la regla de l'Hôpital y definimos
\begin{align*}
Y_{n}(x) &\equiv \lim_{s \to n} Y_{s} (x) = \lim_{s \to n} \dfrac{\displaystyle \pdv{s} \left[ J_{s}(x) \,\cos s \pi - J_{-s}(x) \right]}{\pi \, \cos n \pi} \\[1em]
&= \dfrac{1}{\pi} \lim_{s \to n} \left[ \pdv{J_{s}}{s} - (-1)^{n} \pdv{J_{-s}}{s} \right]
\end{align*}
De la ec. (\ref{eq:ecuacion_27_10}) se obtiene que
\begin{align*}
\pdv{J_{s}}{s} = J_{s} (x) \, \ln \left( \dfrac{x}{2} \right) - \left( \dfrac{x}{2} \right)^{s} \sum_{k=0}^{\infty} (-1)^{k} \, \dfrac{\Psi(s + k + 1)}{k! \, \Gamma(s + k + 1)} \, \left( \dfrac{x}{2} \right)^{2k}
\end{align*}
donde
\begin{align*}
\Psi (x) \equiv \dv{x} \ln[(x - 1)!] = \dv{x} \ln \Gamma (x) = \dfrac{\dv*{\Gamma(x)}{x}}{\Gamma (x)}
\end{align*}
De manera similar
\begin{align*}
\pdv{J_{-s}}{s} = -J_{-s} (x) \, \ln \left( \dfrac{x}{2} \right) + \left( \dfrac{x}{2} \right)^{-s} \sum_{k=0}^{\infty} (-1)^{k} \, \dfrac{\Psi(-s + k + 1)}{k! \, \Gamma(-s + k + 1)} \, \left( \dfrac{x}{2} \right)^{2k}
\end{align*}
Al sustituir estas expresiones en la definición de $Y_{n}(x)$ y usando la identidad $J_{-n} = (-1)^{n} J_{n}(x)$, se obtiene
\begin{align}
\begin{aligned}
Y_{n} (x) &= \dfrac{2}{\pi} \, J_{n} (x) \, \ln \left(\dfrac{x}{2} \right) - \dfrac{1}{\pi} \left( \dfrac{x}{2} \right)^{n} \sum_{k=0}^{\infty} (-1)^{k} \dfrac{\Psi (n + k + 1)}{k! \, \Gamma (n + k + 1)} \left(\dfrac{x}{2} \right)^{2k} + \\
&- \dfrac{1}{\pi} (-1)^{n} \left( \dfrac{x}{2} \right)^{-n} \sum_{k=0}^{\infty} (-1)^{k} \dfrac{\Psi (k - n + 1)}{k! \, \Gamma (k - n + 1)} \left(\dfrac{x}{2} \right)^{2k}
\end{aligned}
\label{eq:ecuacion_27_12}
\end{align}
Debe de quedar claro con la ec. (\ref{eq:ecuacion_27_12}) que la función de Neumann $Y_{s} (x)$ está mal definida en $x = 0$, como se espera de la segunda solución para la ED de Bessel, como la función $Z_{m} (x)$ discutida anteriormente.
\par
Como $Y_{s} (x)$ es linealmente independiente de $J_{s} (x)$ para cualquier $s$, entero o no entero, es conveniente considerar $\left\{ J_{s} (x), Y_{s} (x) \right\}$ como base de las soluciones para la ED de Bessel. En particular, la solución de la ecuación radial en coordenadas cilíndricas, es decir, la primera ecuación en (\ref{eq:ecuacion_27_01}), se convierte en
\begin{equation}
R(\rho) = A \, J_{m}(v) + B \, Y_{m}(v) = A \, J_{m} (l \, \rho) + B \, Y_{m} (l \, \rho)
\label{eq:ecuacion_27_13}
\end{equation}
\section{Propiedades de las funciones de Bessel.}
\subsection{Funciones de Bessel de orden entero negativo.}
La ec. (\ref{eq:ecuacion_11_32}) nos proporciona una relación de la función de Bessel de orden entero y la función de Bessel de orden entero negativo:
\begin{equation}
J_{-m} (x) = (-1)^{m} \, J_{m} (x)
\label{eq:ecuacion_27_14}
\end{equation}
\subsection{Relaciones de recurrencia.}
Una primera relación de recurrencia que no involucra la derivada de la función de Bessel es:
\begin{equation}
J_{m-1} (x) + J_{m+1} (x) = \dfrac{2 \, m}{x} \, J_{m} (x)
\label{eq:ecuacion_27_15}
\end{equation}
La siguiente relación considera la derivada de la función de Bessel:
\begin{equation}
J_{m-1} (x) - J_{m+1} (x) = 2 \, J_{m}^{\prime} (x)
\label{eq:ecuacion_27_16}
\end{equation}
Combinando las dos ecuaciones anteriores, se obtiene
\begin{align}
\begin{aligned}
J_{m-1}(x) &= \dfrac{m}{x} \, J_{m} (x) + J_{m}^{\prime} (x) \\[0.5em]
J_{m+1}(x) &= \dfrac{m}{x} \, J_{m} (x) - J_{m}^{\prime} (x)
\end{aligned}
\label{eq:ecuacion_27_17}
\end{align}
Podemos utilizar estas ecuaciones para obtener nuevas y útiles relaciones de recurrencia. Por ejemplo, al diferenciar $x^{m} \, J_{m} (x)$, se obtiene
\begin{align*}
[x^{m} \, J_{m} (x)]^{\prime} &= m \, x^{m-1} \, J_{m} (x) + x^{m} \, J_{m}^{\prime} (x) \\[1em]
&= x^{m} \underbrace{ \left[ \dfrac{m}{x} \, J_{m} (x) + J_{m}^{\prime} (x)\right]}_{=J_{m-1}(x) \mbox{ por } (\ref{eq:ecuacion_27_17})}  \\[1em]
&= x^{m} \, J_{m-1} (x)
\end{align*}
Integrando esta ecuación, nos lleva a
\begin{equation}
\int x^{m} \, J_{m-1} (x) \dd{x} = x^{m} \, J_{m} (x)
\label{eq:ecuacion_27_18}
\end{equation}
De manera similar, se obtiene que
\begin{equation}
\int x^{-m} \, J_{m+1} (x) \dd{x} = -x^{-m} \, J_{m} (x)
\label{eq:ecuacion_27_19}
\end{equation}
\subsection{Ortogonalidad.}
Las funciones de Bessel satisfacen una relación de ortogonalidad, la cantidad que determina esta ortogonalidad de las diferentes funciones de Bessel no es el orden sino un parámetro en su argumento.
\par
Considere dos soluciones de la ED de Bessel que corresponden al mismo parámetro azimutal, pero con un parámetro radial diferente. Más específicamente, hagamos
\begin{align*}
f(\rho) &= J_{m} (k \rho) \\
g(\rho) &= J_{m} (l \rho)
\end{align*}
Entonces se tiene que
\begin{align*}
\dv[2]{f}{\rho} + \dfrac{1}{\rho} \, \dv{f}{\rho} + \left( k^{2} - \dfrac{m^{2}}{\rho^{2}} \right) \, f &= 0 \\[1em]
\dv[2]{g}{\rho} + \dfrac{1}{\rho} \, \dv{g}{\rho} + \left( l^{2} - \dfrac{m^{2}}{\rho^{2}} \right) \, g &= 0
\end{align*}
Al multiplicar la primera ecuación por $\rho \, g$ y la seguna por $\rho \, f$, para luego restarlar, se llega a
\begin{align*}
\dv{\rho} [\rho (f \, g^{\prime} - g \, f^{\prime} ) ] = (k^{2} - l^{2}) \rho \, f \, g
\end{align*}
donde el primado indica que la diferenciación es con respecto a la variable $\rho$. Al integrar la última ecuación con respecto a $\rho$ de un valor inicial (digamos $a$) a un valor final (digamos $b$), se obtiene
\begin{align*}
[\rho (f \, g^{\prime} - g \, f^{\prime} ) ]\eval_{a}^{b} = (k^{2} - l^{2}) \int_{a}^{b} \rho \, f(\rho) \, g(\rho) \dd{\rho}
\end{align*}
En todas las aplicaciones físicas, tanto $a$ como $b$ se escogen de tal manera para hacer que el lado derecho de la igualdad se anule. Entonces, al sustituir $f$ y $g$ en términos de las funciones de Bessel, tenemos
\begin{align*}
(k^{2} - l^{2}) \int_{a}^{b} \rho \, J_{m} (k \, \rho) \, J_{m}(l \, \rho) \dd{\rho} = 0
\end{align*}
Se sigue que si $k \neq l$, entonces la integral se anula, es decir
\begin{equation}
\int_{a}^{b} \rho \, J_{m} (k \, \rho) \, J_{m}(l \, \rho) \dd{\rho} = 0 \hspace{1.5cm} \mbox{ si } k \neq l
\label{eq:ecuacion_27_20}
\end{equation}
Esta es la relación de ortogonalidad para las funciones de Bessel.
\par
Para completar la relación de ortogonalidad, también debemos abordar el caso cuando $k = l$. Esto implica la evaluación de la integral $\displaystyle \int \rho J_{m}^{2} ( k \, \rho) \dd{\rho}$, la cual con el cambio de variable $x \equiv k \, \rho$ se reduce a
\begin{align*}
\dfrac{\displaystyle \int x \, J_{m}^{2} (x) \dd{x}}{k^{2}}
\end{align*}
Integrando por partes, se tiene
\begin{align*}
I \equiv \int  \underbrace{J_{m}^{2} (x)}_{u} \, \underbrace{x \dd{x}}_{\dd{v}} =  \dfrac{1}{2} x^{2} \, J_{m}^{2} (x)
 - \int J_{m}(x) \, J_{m}^{\prime} (x) \, x^{2} \dd{x}
\end{align*}
En la última integral, sustituimos por $x^{2} \, J_{m} (x)$ de la ecuación de Bessel (\ref{eq:ecuacion_27_06}), usangdo $x$ en lugar de $v$:
\begin{align*}
x^{2} \, J_{m} (x) =  m^{2} \, J_{m}(x) - x \, J_{m}^{\prime} (x) - x^{2} \, J_{m}^{\prime \prime} (x)
\end{align*}
Por lo tanto
\begin{align*}
I &= \dfrac{1}{2} \, x^{2} \, J_{m}^{2} (x) - \int J_{m}^{\prime}(x) \, [m^{2} J_{m}(x) \overbrace{- x \, J_{m}^{\prime} (x) - x^{2} \, J_{m}^{\prime \prime} (x)]}^{=-(\frac{1}{2} x^{2} [J_{m}^{\prime} (x)]^{2})^{\prime}} \, \dd{x} \\[1em]
&= \dfrac{1}{2} \, x^{2} \, J_{m}^{2} (x) - m^{2} \int \overbrace{J_{m} (x) \, J_{m}^{\prime} (x)}^{=\frac{1}{2} [J_{m}^{2}(x)]^{\prime}} \dd{x} + \dfrac{1}{2} \int \dv{x} (x^{2} [J_{m}^{\prime} (x)]^{2}) \dd{x} \\[1em]
&= \dfrac{1}{2} \, x^{2} \, J_{m}^{2} (x) - \dfrac{1}{2} m^{2} \, J_{m}^{2} (x) + \dfrac{1}{2} \, x^{2} \, [J_{m}^{\prime}(x)]^{2}
\end{align*}
Regresando a la variable $\rho$, obtenemos una integral indefinida
\begin{equation}
\int \rho \, J_{m}^{2} ( k \, \rho) \dd{\rho} = \dfrac{I}{k^{2}} = \dfrac{1}{2} \left(\rho^{2} - \dfrac{m^{2}}{k^{2}} \right) \, J_{m}^{2} (k \, \rho) + \dfrac{1}{2} \rho^{2} [J_{m}^{\prime}(k \, \rho)]^{2}
\label{eq:ecuacion_27_21}
\end{equation}
En la mayoría de las aplicaciones, el límite inferior de integración es cero y el límite superior es un número positivo $a$. El lado derecho de la igualdad de la ec. (\ref{eq:ecuacion_27_21}) se anula en el límite inferior debido a la siguiente razón: El primer término se desvanece en $\rho = 0$ porque $J_{m} (0) = 0$ para todo $m > 0$, como se desprende de la expansión de la serie (\ref{eq:ecuacion_27_10}). Para $m = 0$ (y $\rho = 0$), los paréntesis en el primer término de la ec. (\ref{eq:ecuacion_27_21}) se anulan. Entonces, el primer término es cero para todo $m \geq 0$ en el límite inferior de integración. El segundo término se anula debido a la presencia de $\rho^{2}$. Así, obtenemos
\begin{equation}
\int_{0}^{a} \rho \, J_{m}^{2} ( k \, \rho) \dd{\rho} = \dfrac{1}{2} \left( a^{2} - \dfrac{m^{2}}{k^{2}} \right) \, J_{m}^{2} (k \, a) + \dfrac{1}{2} a^{2} [J_{m}^{\prime} (k \, a)]^{2}
\label{eq:ecuacion_27_22}
\end{equation}
para todo $m \geq 0$ y, por la ec. (\ref{eq:ecuacion_27_14}), también para todos los enteros negativos. Como se mencionó anteriormente, limitaremos nuestra discusión a las funciones de Bessel de orden entero. Es costumbre simplificar el lado derecho de la igualdad de la ec. (\ref{eq:ecuacion_27_22}) al elegir $k$ de tal manera que $J_{m} (k \, a) = 0$, es decir, que $k \, a$ sea una raíz de la función de Bessel de orden $m$. En general, hay infinitas raíces. Entonces, digamos que $x_{mn}$ corresponde a la $n$-ésima raíz de $J_m (x)$. Entonces,
\begin{align*}
k \, a = x_{mn} \hspace{1cm} \Longrightarrow \hspace{1cm} k = \dfrac{x_{mn}}{a}, \hspace{1cm} n = 1, 2, 3, \ldots
\end{align*}
Si utilizamos la ec. (\ref{eq:ecuacion_27_17}) obtenemos
\begin{equation}
\int_{0}^{a} \rho \, J_{m}^{2} \left(\dfrac{x_{mn} \, \rho}{a}   \right) \dd{\rho} = \dfrac{1}{2} a^{2} [J_{m+1} (x_{mn})]^{2}
\label{eq:ecuacion_27_23}
\end{equation}
Las ecuaciones (\ref{eq:ecuacion_27_20}) y (\ref{eq:ecuacion_27_23}) se pueden combinar en una ecuación sencilla usando una delta de Kronecker:
\begin{tcolorbox}
Las funciones de Bessel de orden entero satisfacen la relación de ortogonalidad.
\begin{equation}
\int_{0}^{a} J_{m} \left(\dfrac{x_{mn} \, \rho}{a} \right) \, J_{m} \left(\dfrac{x_{mk} \, \rho}{a} \right) \, \rho \dd{\rho} = \dfrac{1}{2} a^{2} \, J_{m+1}^{2} (x_{mn}) \, \delta_{kn}
\label{eq:ecuacion_27_24}
\end{equation}
donde $a > 0$ y $x_{mn}$ es la $n$-ésima raíz de $J_{m}(x)$.
\end{tcolorbox}
\subsection{Función generatriz.}
Las funciones de Bessel de orden entero tienen una función generatriz, es decir, existe una función $g(x, t)$, tal que
\begin{equation}
g (x, t) = \sum_{n=-\infty}^{\infty} t^{n} \, J_{n} (x)
\label{eq:ecuacion_27_25}
\end{equation}
Para determinar a la función $g$, comenzamos con la siguiente relación de recurrencia
\begin{align*}
J_{m-1}(x) + J_{m+1} (x) = \dfrac{2 \, m}{x} \, J_{m} (x)
\end{align*}
multiplicándola por $t^{m}$, para luego sumar sobre todo $m$, tenemos
\begin{equation}
\sum_{m=-\infty}^{\infty} t^{m} \, J_{m-1} (x) + \sum_{m=-\infty}^{\infty} t^{m} \, J_{m+1} (x) = \dfrac{2}{x} \, \sum_{m=-\infty}^{\infty} m \, t^{m} \, J_{m} (x)
\label{eq:ecuacion_27_26}
\end{equation}
La primera suma se puede escribir como
\begin{align*}
\sum_{m=-\infty}^{\infty} t^{m} \, J_{m-1} (x) = t \, \sum_{-\infty}^{\infty} t^{m-1} \, J_{m-1} (x) =  t \, \sum_{-\infty}^{\infty} t^{n} \, J_{n} (x) = t \, g(x, t)
\end{align*}
donde se ha sustituido el índice mudo $n = m -1$ por $m$. De manera similar
\begin{align*}
\sum_{m=-\infty}^{\infty} t^{m} \, J_{m+1} (x) = \dfrac{1}{t} \, \sum_{-\infty}^{\infty} t^{m+1} \, J_{m+1} (x) = \dfrac{1}{t} \,  g(x, t)
\end{align*}
y
\begin{align*}
\dfrac{2}{m} \, \sum_{m=-\infty}^{\infty} m \, t^{m} \, J_{m} (x) = \dfrac{2 \, t}{x} \, \sum_{m=-\infty}^{\infty} m \, t^{m-1} \, J_{m} (x) = \dfrac{2 \, t}{x} \, \pdv{g}{t}
\end{align*}
Se sigue de la ec. (\ref{eq:ecuacion_27_26}) que
\begin{align*}
\left( t + \dfrac{1}{t} \right) \, g(x, t) = \dfrac{2 \, t}{x} \, \pdv{g}{t}
\end{align*}
o también
\begin{align*}
\dfrac{x}{2} \left( 1 + \dfrac{1}{t^{2}} \right) \dd{t} = \dfrac{\dd{g}}{g}
\end{align*}
donde se supone que $x$ es una constante porque hemos estado diferenciando con respecto a $t$. Integrando ambos lados resulta
\begin{align*}
\underbrace{\int \dfrac{x}{2} \left( 1 + \dfrac{1}{t^{2}} \right) \dd{t}}_{=\frac{x}{2} (t - \frac{1}{t})} = \ln g + \ln \phi(x)
\end{align*}
donde el último término es la \enquote{constante} de integración. Entonces
\begin{align*}
g (x, t) = \phi (x) \, \exp \left[ \dfrac{x}{2} \left( t - \dfrac{1}{t} \right) \right]
\end{align*}
Para encontrar $\phi(x)$, veamos que
\begin{align*}
g(x, t) &= \phi (x) \, \exp(x \, t / 2) \, \exp(-x \, t / 2) = \\[1em]
&= \phi (x) \, \sum_{n=0}^{\infty} \dfrac{(x \, t / 2)^{n}}{n!} \, \sum_{m=0}^{\infty} \dfrac{(-x \, t / 2)^{m}}{m!} \\[1em]
&= \phi (x) \, \sum_{n, m=0}^{\infty} \dfrac{(-1)^{m}}{n! \, m!} \, \left( \dfrac{x}{2} \right)^{n+m} \, t^{n-m}
\end{align*}
En la última suma doble, se reúnen todos los términos cuya potencia de $t$ es cero y llamemos a esta suma $S_{0}$. Esto se obtiene haciendo $n = m$. Entonces,
\begin{align*}
S_{0} = \phi (x) \, \sum_{n=0}^{\infty} \dfrac{(-1)^{n}}{n! \, n!} \, \left( \dfrac{x}{2} \right)^{2n} = \phi (x) \, J_{0} (x)
\end{align*}
donde hemos usado la ec. (\ref{eq:ecuacion_27_10}) con $s = 0$. Pero la ec. (\ref{eq:ecuacion_27_05}) nos dice que al juntar los términos cuya potencia de $t$ es cero, es simplemente $J_{0} (x)$. Por lo tanto, $S_{0} = J_{0} (x)$, y $\phi (x) = 1$. Esto nos conduce a la forma final de la función de generación de Bessel:
\begin{equation}
g (x, t) = \exp \left[ \dfrac{x}{2} \left( t - \dfrac{1}{t} \right) \right] = \sum_{n=-\infty}^{\infty} t^{n} \, J_{n} (x)
\label{eq:ecuacion_27_27}
\end{equation}
\textbf{Problema a cuenta: } A partir de la función generatriz, demuestra el \textbf{teorema de adición} de las funciones de Bessel:
\begin{equation}
J_{n} (x + y) = \sum_{m=-\infty}^{\infty} J_{n-m} (x) \, J_{m} (y) = \sum_{m=-\infty}^{\infty} J_{m} (x) \, J_{n-m} (y)
\label{eq:ecuacion_27_28}
\end{equation}
Tip: Considera que
\begin{align*}
g (x + y, t) =  g(x, t) \, g(y, t)
\end{align*}
que tendrías que expandir y realizar las debidas operaciones para llegar al resultado indicado en la ec. (\ref{eq:ecuacion_27_28}).
\par
La función generatriz de Bessel también nos puede conducir a algunas identidades muy importantes. En la ec. (\ref{eq:ecuacion_27_27}), hacemos $t = e^{i \theta}$ y usamos el resultado
\begin{equation}
\sin \theta = \dfrac{1}{2 \, i} (e^{i \theta} - e^{-i \theta})
\label{eq:ecuacion_18_14}
\end{equation}
para obtener
\begin{equation}
e^{i  \, x \, \sin \theta} = \sum_{n=-\infty}^{\infty} e^{i n \theta} \, J_{n} (x)
\label{eq:ecuacion_27_29}
\end{equation}
Esta es una expansión de la serie de Fourier en $\theta$, cuyos coeficientes son las funciones de Bessel. Para encontrar estos coeficientes, multiplicamos ambos lados por $e^{-i m \theta}$ para luego integrar de $-\pi$ a $\pi$. El lado izquierdo de la igualdad nos devuelve
\begin{align*}
\int_{-\pi}^{\pi} e^{i \, x \, \sin \theta} \, e^{i \, m \,  \theta} \dd{\theta} = \int_{-\pi}^{\pi} e^{i (x \, \sin \theta -  m \,  \theta} \dd{\theta}
\end{align*}
Para el lado derecho de la igualdad, tenemos
\begin{align*}
\sum_{n=-\infty}^{\infty} \left[ \int_{-\pi}^{\pi} e^{i(n - m) \theta} \dd{\theta} \right] \, J_{n} (x) = 2 \, \pi \, J_{m}(x)
\end{align*}
En donde hemos utilizado el siguiente resultado (que se puede verificar fácilmente):
\begin{align*}
\int_{-\pi}^{\pi} e^{i(n - m) \theta} \dd{\theta} &= \begin{cases}
0 & \mbox{ si } n \neq m \\
2 \, \pi & \mbox{ si } n = m
\end{cases} \\
&= 2 \pi \, \delta_{mn}
\end{align*}
Al igualar ambos lados de la expresión, obtenemos
\begin{equation}
J_{m} (x) = \dfrac{1}{2 \, \pi} \int_{-\pi}^{\pi} e^{i (x \, \sin \theta -  m \, \theta}) \dd{\theta}
\label{eq:ecuacion_27_30}
\end{equation}
Que podemos reducir (ejericio a cuenta) a
\begin{equation}
J_{m} (x) = \dfrac{1}{\pi} \int_{-\pi}^{\pi} \cos (x \, \sin \theta -  m \, \theta) \dd{\theta}
\label{eq:ecuacion_27_31}
\end{equation}
que es llamada la \textbf{integral de Bessel.}
\section{Expansión mediante funciones de Bessel.}
La ortogonalidad de las funciones de Bessel puede ser útil para ampliar otras funciones en términos de ellas. La idea básica es similar a la expansión en las series de Fourier. Si una función $f(\rho)$ está definida en el intervalo $(0, a)$, entonces podemos escribir
\begin{equation}
f(\rho) = \sum_{n=1}^{\infty} c_{n} \, J_{m} \left( \dfrac{x_{mn}\, \rho}{a} \right)
\label{eq:ecuacion_27_34}
\end{equation}
Los coeficinetes $c_{n}$ pueden calcularse al multiplicar ambos lados por la expresión $\rho \, J_{m}(x_{mk} \, \rho/a)$, para luego integrar de $0$ a $a$, se puede verificar que esto nos lleva abordar
\begin{equation}
c_{n} = \dfrac{2}{a^{2} \, J_{m+1}^{2} (x_{mn})} \, \int_{0}^{a} f (\rho) \, J_{m} \left( \dfrac{x_{mn} \, \rho}{a} \right) \, \rho \dd{\rho}
\label{eq:ecuacion_27_35}
\end{equation}
Con estas ecuaciones, podemos expandir una función en términos de las funciones de Bessel de un orden en particular.
\par
Si elevamos al cuadrado la ec. (\ref{eq:ecuacion_27_34}) y la multiplicamos por $\rho$, para luego integrar de $0$ a $a$, se tiene que
\begin{align*}
\int_{0}^{a} f^{2} (\rho) \, \rho \dd{\rho} = \sum_{n=1}^{\infty} \sum_{k=1}^{\infty} c_{n} \, c_{k} \, \underbrace{\int_{0}^{a} J_{m} \left( \dfrac{x_{mn} \, \rho}{a} \right) \, J_{m} \left( \dfrac{x_{mk} \, \rho}{a} \right) \, \rho \dd{\rho}}_{=\frac{1}{2} a^{2} J_{m+1}^{2} (x_{mn}) \delta_{kn} \mbox{ por } (\ref{eq:ecuacion_27_24})}
\end{align*}
que nos conduce a la llamada \textbf{relación de Parseval}.
\begin{equation}
\int_{0}^{a} f^{2} (\rho) \, \rho \dd{\rho} = \dfrac{1}{2} a^{2} \sum_{n=1}^{\infty} c_{n}^{2} \, J_{m+1}^{2} (x_{mn})
\label{eq:ecuacion_27_38}
\end{equation}
para un valor de $m$. Este valor de $m$ se puede escoger para que las integrales sean lo más sencillas posible.
\section{Ejemplos de la física.}
Podemos considerar que las funciones de Bessel son las funciones \enquote{naturales} de la geometría cilíndrica.
\par
Como en el caso de las coordenadas cartesianas y esféricas, a menos que la simetría del problema simplifique la situación, la separación de la ecuación de Laplace da como resultado dos parámetros que conducen a una suma doble. Así, esperamos una suma doble en la solución más general de la ecuación de Laplace en geometrías cilíndricas. Una de estas sumas es sobre $m$ que, como muestra la ecuación (\ref{eq:ecuacion_27_03}), aparece en el argumento de las funciones seno y coseno. También designa el orden de la función Bessel (o Neumann).
\par
\subsection{Potencial en un cilindro conductor.}
Para comprender el origen de la segunda suma, considera un cilindro conductor, de radio $a$ y altura $h$ (ver la figura xxx). Supongamos que el potencial en la tapa superior varía como $V (\rho, \varphi)$ mientras que la superficie lateral y la tapa inferior están conectadas a tierra.
\par
Calculemos el potencial electrostático $\Phi$ en todos los puntos dentro del cilindro.
\par
La solución general es el producto de las soluciones (\ref{eq:ecuacion_27_03}), (\ref{eq:ecuacion_27_04}) y (\ref{eq:ecuacion_27_13}):
\begin{align*}
\Phi (\rho, \varphi, z) = R(\rho) \, S(\varphi) \, Z(z)
\end{align*}
Como $\Phi (\rho, \varphi, 0) = 0$ para cualquier valor arbitrario de $\rho$ y $\varphi$, tenemos que $Z(0) = 0$, dando una constante $Z(z) = \sinh (l \, z)$.
\par
Ya que $\Phi (0, \varphi, z)$ es finito, es finito, no se permite ninguna función de Neumann en la expansión y, dentro de una constante, tenemos $R(\rho) = J_{m}(l \, \rho)$. Además, como $\Phi (a, \varphi, z) = 0$ para cualquier valor arbitrario $\varphi$ y $z$, debemos de tener
\begin{align*}
R(a) = J_{m} (l \, a) = 0 \hspace{0.5cm} \Longrightarrow \hspace{0.5cm} l \, a = x_{mn} \hspace{0.5cm} \Longrightarrow \hspace{0.5cm} l = \dfrac{x_{mn}}{a} \hspace{1cm} n = 1, 2, \ldots
\end{align*}
donde, $x_{mn}$ son las $n$-ésimas raíces de $J_{m}$.
\par
Ahora podemos multiplicar $R$, $S$ y $Z$ y luego sumar todos los valores posibles de $m$ y $n$, teniendo en cuenta que los valores negativos de $m$ dan términos que dependen linealmente de los valores positivos correspondientes. El resultado es la llamada \textbf{serie de Fourier-Bessel}:
\begin{equation}
\Phi (\rho, \varphi, z) = \sum_{m=0}^{\infty} \, \sum_{n=1}^{\infty} J_{m} \left( \dfrac{x_{mn}}{a}  \, \rho \right) \, \sinh \left( \dfrac{x_{mn}}{a}  \, z \right) \, (A_{mn} \cos m \, \varphi + B_{mn} \sin m \, \varphi )
\label{eq:ecuacion_27_39}
\end{equation}
donde las constantes $A_{mn}$ y $B_{mn}$ deben ser determinadas por las CDF restantes que establecen que $\Phi (\rho, \varphi, z) = V(\rho, \varphi)$ o
\begin{equation}
V (\rho, \varphi) = \sum_{m=0}^{\infty} \, \sum_{n=1}^{\infty} J_{m} \left( \dfrac{x_{mn}}{a}  \, \rho \right) \, \sinh \left( \dfrac{x_{mn}}{a}  \, h \right) \, (A_{mn} \cos m \, \varphi + B_{mn} \sin m \, \varphi )
\label{eq:ecuacion_27_40}
\end{equation}
Multiplicando ambos lados por
\begin{align*}
\rho \, J_{m} \left( \dfrac{x_{mk} \, a}{\rho} \right) \, \cos j \varphi
\end{align*}
para luego integrar de $0$ a $2 \, \pi$, y de $0$ a $a$ en $\rho$, se obtiene $A_{jk}$. Cambiando el coseno al seno, y siguiendo el mismo procedimiento, obtenemos $B_{jk}$. Regresamos del índice $m$ al $n$, por lo tanto
\begin{align}
\begin{aligned}
A_{mn} &= \dfrac{\displaystyle 2 \int_{0}^{2 \pi} \dd{\varphi} \int_{0}^{a} \rho \, V (\rho, \varphi) \, J_{m} \, \left( \dfrac{x_{mn}}{a}  \, \rho \right) \, \cos m \varphi \dd{\rho}}{\pi \, a^{2} \, J_{m+1}^{2} (x_{mn}) \, \sinh \left( \dfrac{x_{mn} \, h}{a} \right) } \\[1em]
B_{mn} &= \dfrac{\displaystyle 2 \int_{0}^{2 \pi} \dd{\varphi} \int_{0}^{a} \rho \, V (\rho, \varphi) \, J_{m} \, \left( \dfrac{x_{mn}}{a}  \, \rho \right) \, \sin m \varphi \dd{\rho}}{\pi \, a^{2} \, J_{m+1}^{2} (x_{mn}) \, \sinh \left( \dfrac{x_{mn} \, h}{a} \right) }
\end{aligned}
\label{eq:ecuacion_27_41}
\end{align}
en donde se ha utilizado la ec. (\ref{eq:ecuacion_27_24}).
\par
El caso importante de simetría azimutal requiere una consideración especial. En tal caso, el potencial de la superficie superior $V (\rho, \varphi)$ debe ser independiente de $\varphi$. Además, como $S (\varphi)$ es constante, su derivada debe anularse. Por lo tanto, la segunda ecuación en (\ref{eq:ecuacion_27_01}) devuelve $\mu = -m^{2} = 0$. Este valor cero de $m$ reduce la suma doble de la ec. (\ref{eq:ecuacion_27_39}) a una sola suma, y obtenemos
\begin{equation}
\Phi (\rho, z) = \sum_{n=1}^{\infty} A_{n} \, J_{0} \left( \dfrac{x_{0n}}{a}  \, \rho \right) \, \sinh \left( \dfrac{x_{0n}}{a}  \, z \right)
\label{eq:ecuacion_27_42}
\end{equation}
Los coeficientes $A_{n}$ pueden obtenerse haciendo $m = 0$ en la primera ecuación de (\ref{eq:ecuacion_27_41}):
\begin{equation}
A_{n} = \dfrac{4}{a^{2} \, J_{1}^{2} (x_{0n}) \, \sinh \left( \dfrac{x_{0n} \, h}{a} \right)} \, \int_{0}^{a} \rho \, V (\rho) \, J_{0} \, \left( \dfrac{x_{0n}}{a}  \, \rho \right) \dd{\rho}
\label{eq:ecuacion_27_43}
\end{equation}
donde $V(\rho)$ es el potencial independiente de $\varphi$ en la tapa superior del cilindro.
\subsection{Cilindro conductor con tapa a un potencial fijo $V_{0}$.}
Supongamos que la tapa superior de un cilindro conductor se mantiene a un potencial constante $V_{0}$ mientras que la superficie lateral y la tapa inferior están conectadas a tierra.
\par
Queremos calcular el potencial electrostático $\Phi$ en todos los puntos dentro del cilindro.
\par
Como el potencial de la tapa superior es independiente de $\varphi$, prevalece la simetría azimutal y la ec. (\ref{eq:ecuacion_27_43}) nos devuelve
\begin{align*}
A_{n} &= \dfrac{4 \, V_{0}}{a^{2} \, J_{1}^{2} (x_{0n}) \, \sinh \left( \dfrac{x_{0n} \, h}{a} \right)} \, \int_{0}^{a} \rho \,  J_{0} \, \left( \dfrac{x_{0n}}{a}  \, \rho \right) \dd{\rho} \\[1em]
&= \dfrac{4 \, V_{0}}{x_{0n} \, J_{1} (x_{0n}) \, \sinh \left( \dfrac{x_{0n} \, h}{a} \right)}
\end{align*}
donde hemos usado la ec. (\ref{eq:ecuacion_27_18}). El detalle del cálculo de la integral es \textbf{un problema a cuenta}. Se tiene entonces que
\begin{align*}
\Phi (\rho, z) = 4 \, V_{0} \, \sum_{n=1}^{\infty} \dfrac{J_{0} \left( \dfrac{x_{0n} \, \rho}{a} \right) \, \sinh \left( \dfrac{x_{0n} \, z}{a} \right)}{{x_{0n} \, J_{1} (x_{0n}) \, \sinh \left( \dfrac{x_{0n} \, h}{a} \right)}}
\end{align*}
\end{document}