\input{../preambulo_doc}
\title{Método de Frobenius \\ \large {Tema 2 - Matemáticas Avanzadas de la Física}\vspace{-1.5\baselineskip}}
\date{ }
\author{}
\begin{document}
\maketitle
\fontsize{14}{14}\selectfont
%Referencia Arfken - 9.4 Puntos singulares 6th. ed.
\section{Puntos singulares}
En esta parte del Tema 2, introducimos el concepto de un punto singular o singularidad (tal como se aplica a una ecuación diferencial). El interés en este concepto radica en su utilidad en
\begin{enumerate}
\item Clasificar las ODE.
\item Revisar la viabilidad de una solución en series, esta viabilidad es parte del teorema de Fuchs.
\end{enumerate}
\par
Las EDO que hemos mencionado previamente, pueden resolverse para $\displaystyle \dv[2]{y}{x}$, usando la notación $\displaystyle \dv[2]{y}{x} = y^{\prime \prime}$, tenemos:
\begin{align}
y^{\prime \prime} = f(x, y, y^{\prime})
\label{eq:ecuacion_09_74}
\end{align}
Ahora bien, si en la ec. (\ref{eq:ecuacion_09_74}) $y$ e $y^{\prime}$ pueden tener todos los valores finitos a $x = x_{0}$ e $y^{\prime \prime}$ permanece finita, el punto $x = x_{0}$ es un \emph{punto ordinario}. Por otra parte, si $y^{\prime \prime}$ se vuelve infinita para cualquier selección finita de $y$ e  $y^{\prime \prime}$, el punto $x = x_{0}$ se denomina \emph{punto singular}.
\par
Si escribimos este EDO2H (en $y$) como
\begin{align}
y^{\prime \prime} + P(x) \: y^{\prime} + Q(x) \: y = 0
\label{eq:ecuacion_09_75}
\end{align}
Ahora bien, si las funciones $P(x)$ y $Q(x)$ permanecen finitas a $x = x_{0}$, el punto $x = x_{0}$ es un \emph{punto ordinario}. Al contrario, si $P(x)$ y/o $Q(x)$ divergen mientras $x \to x_{0}$, el punto $x_{0}$ es un \emph{punto singular}.
\par
Usando la ecuación (\ref{eq:ecuacion_09_75}) podemos distinguir entre dos tipos de puntos singulares:
\begin{enumerate}
\item Si $P(x)$ y/o $Q(x)$ divergen a medida que $x \to x_{0}$, pero $(x - x_{0}) \: P(x)$ y $(x - x_{0})^{2} \: Q(x)$ permanecen finitas a medida que $x \to x_{0}$, entonces el punto $x = x_{0}$ se llama \textbf{punto singular regular o punto singular no esencial}.
\item Si $P(x)$ diverge más rápidamente que $\dfrac{1}{(x - x_{0})}$, de tal modo que $(x - x_{0}) \: P(x)$ tiene a infinito a medida que $x \to x_{0}$, o cuando $Q(x)$ diverge más rápidamente que $\dfrac{1}{(x - x_{0})^{2}}$, de modo que $(x - x_{0})^{2} \: Q(x)$ tiene a infinito, a medida que $x \to x_{0}$, entonces el punto $x = x_{0}$ se llama \textbf{singularidad esencial o singularidad irregular}.
\end{enumerate}
\par
Estas definiciones son válidas para todos los valores finitos de $x_{0}$. El análisis de los puntos al infinito $(x \to \infty)$ es similar al tratamiento que se hace para las funciones en variable compleja. Hacemos el cambio de variable $x = 1/z$, sustituyendo en la ED y entonces hacemos que $z \to 0$. Haciendo el cambio de variable en las derivadas:
\begin{align}
\dv{y(x)}{x} = \dv{y(z^{-1})}{z} \: \dv{z}{x} = - \dfrac{1}{x^{2}} \dv{y(z^{-1})}{z} = -z^{2} \: \dv{y(z^{-1})}{z}
\label{eq:ecuacion_09_76}
\end{align}
\begin{align}
\begin{aligned}
\dv[2]{y(x)}{x} = \dv{z} \left[ \dv{y(x)}{x} \right] \dv{z}{x} &= (-z^{2}) \left[ -2 \: z \dv{y(z^{-1})}{z} - z^{2} \: \dv[2]{y(z^{-1})}{z} \right] = \\
&= 2 \: z^{3} \: \dv{y(z^{-1})}{z} + z^{4} \: \dv[2]{y(z^{-1})}{z}
\end{aligned}
\label{eq:ecuacion_09_77}
\end{align}
Usando estos resultados, podemos transformar la ecuación (\ref{eq:ecuacion_09_75}) en
\begin{align}
z^{4} \: \dv[2]{y}{z} + [ 2 \: z^{3} - z^{2} \: P(z^{-1})] \: \dv{y}{z} + Q(z^{-1}) \: y = 0
\label{eq:ecuacion_09_78}
\end{align}
El comportamiento en $x = \infty, (z = 0)$ entonces dependerá del comportamiento de los nuevos coeficientes
\begin{align*}
\dfrac{2 \: z - P(z^{-1})}{z^{2}} \hspace{1cm} \text{ y } \hspace{1cm} \dfrac{Q(z^{-1})}{z^{4}}
\end{align*}
a medida que $z \to 0$.
\par
Si estas dos expresiones se mantienen finitas, el punto $x = \infty$ es un punto ordinario. Si las expresiones divergen con mayor rapidez que $1/z$ y $1/z^{2}$, respectivamente, el punto $x = \infty$ es un punto regular singular, de otra manera, el punto es irregular singular (una singularidad esencial).
\subsection*{Ejemplo: La ecuación de Bessel}
La ecuación de Bessel es
\begin{align}
x^{2} \: y^{\prime \prime} + x \: y^{\prime} + (x^{2} - n^{2}) \: y = 0
\label{eq:ecuacion_09_79}
\end{align}
Comparando contra la ecuación (\ref{eq:ecuacion_09_75}), tenemos que
\begin{align*}
P(x) =  \dfrac{1}{x} \hspace{2cm} Q(x) = 1 - \dfrac{n^{2}}{x^{2}}
\end{align*}
lo que demuestra que el punto $x = 0$ es una singularidad regular.
\par
Por inspección vemos que no hay otros puntos singulares en el rango finito. A medida que $x \to \infty (z \to 0)$ de la ecuación (\ref{eq:ecuacion_09_78}) tenemos los coeficientes
\begin{align*}
\dfrac{2 \:z - z}{z^{2}} \hspace{2cm} \dfrac{1 - n^{2} \: z^{2}}{z^{4}}
\end{align*}
ya que la última expresión diverge como $z^{4}$, el punto $x = \infty$ es una singularidad irregular o esencial.
\section{Método de Frobenius}
En esta parte desarrollaremos un método para obtener una solución de la EDO2H lineal. El método, que es un desarrollo en series, funcionará siempre y cuando el punto de expansión no es tan malo que un punto singular regular. En física, esta condición casi siempre satisface.
\par
Una EDO2H lineal puede expresarse de la forma:
\begin{align}
\setlength{\fboxsep}{3\fboxsep}\boxed{\dv[2]{y}{x} + P(x) \: \dv{y}{x} + Q(x) \: y = 0}
\label{eq:ecuacion_09_80}
\end{align}
Esta ecuación es homogénea, lineal y sin productos entre la función $y$ y sus derivadas. Con este método de Frobenius, se obtendrá al menos una solución de la ecuación.
\par
Más adelante veremos que se puede obtener una segunda solución independiente, y se demostrará que no existe una tercera solución independiente.
\par
La solución más general para la ecuación (\ref{eq:ecuacion_09_80}) se expresa por
\begin{align}
\setlength{\fboxsep}{3\fboxsep}\boxed{y(x) = c_{1} \: y_{1}(x) + c_{2} \: y_{2}}
\label{eq:ecuacion_09_81}
\end{align}
En la realidad de la física, el problema nos puede conducir a una EDO2 lineal no homogénea:
\begin{align}
\setlength{\fboxsep}{3\fboxsep}\boxed{\dv[2]{y}{x} + P(x) \: \dv{y}{x} + Q(x) \: y = F(x)}
\label{eq:ecuacion_09_82}
\end{align}
La función de la derecha, $F(x)$, representa una fuente (tal como una carga electrostática) o una fuerza de desplazamiento (como en el oscilador mecánico). Las soluciones específicas de esta ecuación no homogénea se pueden obtener usando las técnicas de la función de Green, y con la técnica de transformada de Laplace que se verá más adelante en el curso.
\par
Al llamar a este solución $y_{p}$, podemos agregarla en cualquier solución de la ecuación homogénea correspondiente (Ec. \ref{eq:ecuacion_09_82}). Por lo tanto \textbf{la solución más general} de la ecuación (\ref{eq:ecuacion_09_82}) es
\begin{align}
\setlength{\fboxsep}{3\fboxsep}\boxed{y(x) = c_{1} \: y_{1}(x) + c_{2} \: y_{2} + y_{p} (x) }
\label{eq:ecuacion_09_83}
\end{align}
Las constantes $c_{1}$ y $c_{2}$ normalmente se establecen por las CDF.
\par
Para nuestro estudio, suponemos que $F(x) = 0$ por lo que nuestra ecuación diferencial es homogénea.
\par
Intentaremos desarrollar una solución de nuestra EDO2H, la Ec. (\ref{eq:ecuacion_09_80}), mediante la sustitución en una serie de potencias con coeficientes indeterminados. Se manejará como parámetro si la potencia del menor del término de la serie es no nulo. Para ilustrar esto, veamos el método de dos ecuaciones diferenciales importantes, la primera es la ecuación del oscilador lineal
\begin{align}
\dv[2]{y}{x} + \omega^{2} \: y = 0
\label{eq:ecuacion_09_84}
\end{align}
de la que conocemos sus soluciones: $y= \sin \omega \: x, \cos \omega \: x$.
\par
Intentamos con
\begin{align}
\begin{aligned}
y(x) &= x^{k} (a_{0} + a_{1} \: x + a_{2} \: x^{2} + a_{3} \: x^{3} + \ldots ) \\
&= \sum_{\lambda = 0}^{\infty} a_{\lambda} \: x^{k + \lambda}, \hspace{1cm} a_{0} \neq 0
\end{aligned}
\label{eq:ecuacion_09_85}
\end{align}
donde el exponente $k$ y todos los coeficientes $a_{\lambda}$ son indeterminados. Nótese que no necesariamente $k$ es un entero. Diferenciando dos veces, tenemos
\begin{align*}
\dv{y}{x} &= \sum_{\lambda=0}^{\infty} a_{\lambda} \: (k + \lambda) x^{k + \lambda - 1} \\[0.5em]
\dv[2]{y}{x} &= \sum_{\lambda=0}^{\infty} a_{\lambda} \: (k + \lambda) (k + \lambda - 1) \: x^{k + \lambda - 2}
\end{align*}
Al sustituir en la ecuación (\ref{eq:ecuacion_09_84}), obtenemos
\begin{align}
\sum_{\lambda=0}^{\infty} a_{\lambda} \: (k + \lambda) \: (k + \lambda - 1) \: x^{k + \lambda - 2} + \omega^{2} \: \sum_{\lambda = 0}^{\infty} a_{\lambda} \: x^{k + \lambda} = 0
\label{eq:ecuacion_09_86}
\end{align}
Del análisis de la unicidad de las series de potencias, los coeficientes de cada potencia de $x$ en la parte izquierda de la ecuación (\ref{eq:ecuacion_09_86}) deben de anularse.
\par
La potencia menor de $x$ que aparece en la ecuación (\ref{eq:ecuacion_09_86}) es $x^{k - 2}$ para $\lambda = 0$ en la primera suma. Para que el coeficiente se anule, se necesita que
\begin{align*}
a_{0} \: k \: (k - 1) = 0
\end{align*}
Se escoge $a_{0}$ como el coeficiente no nulo del término menor de la serie \ref{eq:ecuacion_09_85}, por lo que de la definición, $a_{0} \neq 0$, por lo que tenemos
\begin{align}
 k \: (k - 1) = 0
\label{eq:ecuacion_09_87}
\end{align}
Esta ecuación, que proviene del coeficiente de la potencia más baja de $x$, se llama la \emph{ecuación indicial} o \emph{ecuación de índices}. La ecuación indicial y sus raíces son muy importantes en este análisis. En este ejemplo debemos observar que $k = 0$ o $k = 1$.
\par
Si $k = 1$, el coeficiente $a_{1} \: (k + 1)k$ de $x^{k - 1}$ se anula, por lo que $a_{1} = 0$, en este ejemplo se nota de inmediato que $k = 0$ o $k = 1$.
\par
Antes de considerar estas dos posibilidades para $k$, regresemos a la ecuación (\ref{eq:ecuacion_09_86}) y la condición de que los coeficientes netos restantes, digamos, los coeficientes de $x^{k + j} \; (j \geq 0)$, se anulen.
\par
Hemos establecido $ \lambda = j + 2$ en la primera suma y $\lambda = j$ en el segunda. (Son sumas independientes y $\lambda$ es un índice mudo.) Esto da lugar a
\begin{align*}
a_{j + 2} \: (k + j + 2) \: (k + j + 1) + \omega^{2} \: a_{j} = 0
\end{align*}
o
\begin{align}
a_{j + 2} = - a_{j} \dfrac{\omega^{2}}{(k + j + 2) \: (k + j + 1)}
\label{eq:ecuacion_09_88}
\end{align}
Esta es una \emph{relación de recurrencia} de dos términos: dado $a_{j}$ podemos calcular $a_{j + 2}$ y luego $a_{j + 4}$, $a_{j + 6}$, y así sucesivamente hasta donde lo queramos. Tomemos en cuenta que para este ejemplo, si partimos con $a_{0}$, la ec. (\ref{eq:ecuacion_09_88}) nos conduce a los coeficientes pares $a_{2}$, $a_{4}$ y así sucesivamente, y no considera a $a_{1}$, $a_{3}$, $a_{5}, \ldots$. Dado que una $a_{1}$ es arbitrario, si $k = 0$ y necesariamente cero si $k = 1$, hacemos que sea igual a cero  y luego por la Ec. (\ref{eq:ecuacion_09_88})
\begin{align*}
a_{3} = a_{5} = a_{7} = \ldots = 0
\end{align*}
todos los coeficientes impares se anulan. Las potencias pares de $x$ se presentan cuando se utiliza la segunda raíz de la ecuación indicial.
\par
Regresando a la ecuación (\ref{eq:ecuacion_09_87}) de la ecuación indicial, intentamos con la solución $k = 0$, la relación de recurrencia (Ec. \ref{eq:ecuacion_09_88})ahora es
\begin{align}
a_{j + 2} = - a_{j} \: \dfrac{\omega^{2}}{(j+2) \:(j+1)}
\label{eq:ecuacion_09_89}
\end{align}
que nos conduce a
\begin{align*}
a_{2} &= - a_{0} \: \dfrac{\omega^{2}}{1 \cdot 2} = - \dfrac{\omega^{2}}{2!} \: a_{0} \\
a_{4} &= - a_{2} \dfrac{\omega^{2}}{3 \cdot 4} = + \dfrac{\omega^{4}}{4!} \: a_{0} \\
a_{6} &= - a_{4} \dfrac{\omega^{2}}{5 \cdot 6} = - \dfrac{\omega^{6}}{6!} \: a_{0} \hspace{1cm} \mbox{ y así sucesivamente}
\end{align*}
Aplicando inducción matemática, tenemos que
\begin{align}
a_{2 n} = (-1)^{n} \: \dfrac{\omega^{2 n}}{(2 \: n)!} \: a_{0}
\label{eq:ecuacion_09_90}
\end{align}
y la solución es
\begin{align}
y(x)_{k = 0} = a_{0} \: \left[ 1 - \dfrac{(\omega \: x)^{2}}{2!} + \dfrac{(\omega \: x)^{4}}{4!} - \dfrac{(\omega \: x)^{6}}{6!} + \ldots \right] = a_{0} \: \cos \omega x
\label{eq:ecuacion_09_91}  
\end{align}
Si elegimos la raíz $k = 1$ de la ecuación indicial (Ec. \ref{eq:ecuacion_09_88}), la relación de recurrencia es
\begin{align}
a_{j + 2} = - a_{j} \: \dfrac{\omega^{2}}{(j+3) \: (j+2)}
\label{eq:ecuacion_09_92}
\end{align}
sustituyendo en $j = 0, 2, 4$ sucesivamente, resulta
\begin{align*}
a_{2} &= - a_{0} \: \dfrac{\omega^{2}}{2 \cdot 3} = - \dfrac{\omega^{2}}{3!} \: a_{0} \\
a_{4} &= - a_{2} \dfrac{\omega^{2}}{4 \cdot 5} = + \dfrac{\omega^{4}}{5!}  \: a_{0} \\
a_{6} &= - a_{4} \dfrac{\omega^{2}}{6 \cdot 7} = - \dfrac{\omega^{6}}{7!} \: a_{0} \hspace{1cm} \mbox{y así sucesivamente}
\end{align*}
Nuevamente por inspección e inducción matemática
\begin{align}
a_{2 n} = (-1)^{n} \: \dfrac{\omega^{2 n}}{(2 \: n + 1)!} \: a_{0}
\label{eq:ecuacion_09_93}
\end{align}
Para este valor $k = 1$, se obtiene
\begin{align}
y(x)_{k = 1} &= a_{0} \: x \: \left[ 1 - \dfrac{(\omega \: x)^{2}}{3!} + \dfrac{(\omega \: x)^{4}}{5!} - \dfrac{(\omega \: x)^{6}}{7!} + \ldots \right] \nonumber \\[0.5em]
&= \dfrac{a_{0}}{\omega} \: \left[ (\omega \: x) - \dfrac{(\omega \: x)^{3}}{3!} + \dfrac{(\omega \: x)^{5}}{5!} - \dfrac{(\omega \: x)^{7}}{7!} + \ldots \right] \nonumber \\[0.5em]
&= \dfrac{a_{0}}{\omega} \: \sin \omega x
\label{eq:ecuacion_09_94}
\end{align}
Esta sustitución en series de potencias, es conocida como el \emph{método de Frobenius}, y nos ha dado dos soluciones en series de la ecuación del oscilador lineal. Sin embargo, hay que considerar dos puntos sobre dichas soluciones en series en los que se debe de hacer énfasis:
\begin{enumerate}
\item La solución en series siempre debe de sustituirse nuevamente en la ecuación diferencial, para ver si funciona, como medida de precaución contra los errores algebraicos y de lógica. Si funciona, es una solución.
\item Aceptar una solución en series depende de su convergencia (incluida la convergencia asintótica). Es muy posible que el método de Frobenius devuelva una solución en series que satisface la ecuación diferencial original cuando se sustituye en la ecuación, pero que no converga en el intervalo de interés. (Como en el caso de la ED de Legendre)
\end{enumerate}
%Referencia Echeverria - Tema 4
% \subsection{Teorema de Frobenius.}
% El Teorema de Frobenius permite hallar al menos una solución en forma de serie de potencias para la ecuación 
% \begin{equation}
% y^{\prime \prime} + p(x) \: y^{\prime} + q(x) \: y = 0
% \label{eq:ecuacion_04_02_04}
% \end{equation}
% alrededor de $x_{0}$ cuando el punto es un punto singular regular.
% \par
% Entonces la ecuación (\ref{eq:ecuacion_04_02_04}) posee al menos una solución de la forma
% \begin{equation}
% y(x) = (x - x_{0})^{m} \: \sum_{k=0}^{\infty} c_{k} \: (x - x_{0})^{k}
% \label{eq:ecuacion_04_02_05}
% \end{equation}
% donde $m$ es un número por determinar. Tal serie converge en el intervalo común de convergencia de
% \begin{equation}
% P(x) \equiv (x - x_{0}) \: p(x) \hspace{1cm} Q(x) \equiv (x - x_{0})^{2} \: q(x)
% \label{eq:ecuacion_04_02_06}
% \end{equation}
% excepto quizás en el punto $x = x_{0}$.
% \par
% Sin pérdida de generalidad, puede tomarse $x_{0} = 0$ pues siempre puede realizarse un cambio de variable o traslación para centrar el problema alrededor del origen. En tal caso, para resolver la ec. (\ref{eq:ecuacion_04_02_04}) primero se escribe en función de la ec. (\ref{eq:ecuacion_04_02_06}) como
% \begin{equation}
% y^{\prime \prime} + \dfrac{P(x)}{x} \: y^{\prime} + \dfrac{Q(x)}{x^{2}} \: y = 0
% \label{eq:ecuacion_04_02_07} 
% \end{equation}
% luego puede multiplicarse por $x^{2}$ a ambos lados para obtener la ecuación
% \begin{equation}
% x^{2} \: y^{\prime \prime} + x \: P(x) \: y^{\prime} + Q(x) \: y = 0
% \label{eq:ecuacion_04_02_08} 
% \end{equation}
% Luego, como $P(x)$ y $Q(x)$ son analíticas alrededor de cero puede escribirse
% \begin{equation}
% P(x) \equiv \sum_{k=0}^{\infty} a_{k} \: x^{k} \hspace{1cm} Q(x) \equiv \sum_{k=0}^{\infty} b_{k} \: x^{k}
% \label{eq:ecuacion_04_02_09}
% \end{equation}
% y sustituyendo en la ec. (\ref{eq:ecuacion_04_02_08}), se escribe
% \begin{equation}
% x^{2} \: y^{\prime \prime} + \sum_{k=0}^{\infty} a_{k} \: x^{k+1} \: y^{\prime} + \sum_{k=0}^{\infty} b_{k} \: x^{k} \: y = 0
% \label{eq:ecuacion_04_02_10}
% \end{equation}
% Por el Teorema de Frobenius se busca una solución de la forma
% \begin{equation}
% y(x) = x^{m} \: \sum_{k=0}^{\infty} c_{k} \: x^{k} =  (c_{0} \: x^{m} + c_{1} \: x^{m+1} + c_{2} \: x^{m+2} + \ldots )
% \label{eq:ecuacion_04_02_11}
% \end{equation}
% sin pérdida de generalidad puede tomarse $c_{0} \neq 0$ pues de lo contrario puede factorizarse un $x$ en la serie y escribir $x^{m+1}$ en vez de $x^{m}$. Sustituyendo la ec. (\ref{eq:ecuacion_04_02_11}) en la ec. (\ref{eq:ecuacion_04_02_10}) se obtiene
% \begin{align}
% \begin{aligned}
%  &{} x^{2} \, (m (m-1) \, c_{0} \, x^{m-2} + (m+1) m \, c_{1} \, x^{m-1} + (m+2)(m+1) \, c_{2} \, x^{m} + \ldots ) \\
%  &+ (a_{0} \, x + a_{1} \, x^{2} + a_{2} \, x^{3} + \ldots + )(m \, c_{0} \, x^{m-1} + (m+1) \, c_{1} \, x^{m} + (m+2) \, c_{2} \, x^{m+1} + \ldots ) \\
%  &+ (b_{0} + b_{1} \, x + b_{2} \, + \ldots )(c_{0} \, x^{m} + c_{1} \, x^{m+1} + c_{2} \, x^{m+2} + \ldots ) = 0
% \end{aligned}
% \label{eq:ecuacion_04_02_12}
% \end{align}
% se puede observar que la menor potencia de $x$ que aparece al realizar los productos respectivos es $x^{m-2}$, de hecho si se agrupan los términos según la potencia de $x$, el coeficiente que multiplica a $x^{m-2}$ es
% \begin{equation}
% m(m-1) \: c_{0} + m \: a_{0} \: c_{0} + b_{0} \: c_{0}
% \label{eq:ecuacion_04_02_13}
% \end{equation}
% dado que $c_{0} \neq 0$ y la serie de potencias está igualada a cero, cada coeficiente que multiplica a cada potencia de $x$ debe ser cero, en particular, el coeficiente que multiplica a $x^{m-2}$ debe ser cero, lo cual significa que $m$ debe cumplir la ecuación
% \begin{equation}
% m (m-1) + m \: a_{0} + b_{0} = 0
% \label{eq:ecuacion_04_02_14}
% \end{equation}
% La ecuación (\ref{eq:ecuacion_04_02_14}) se llama la \emph{ecuación indicial}. Dado que es una ecuación cuadrática, en general hay dos raíces $m_{1}$ y $m_{2}$. Dependiendo de tales raíces, el método de Frobenius garantiza una segunda solución.
% \subsection{Casos especiales.}
% \subsubsection{Raíces con diferencia no entera.}
% Si $m_{1}$, $m_{2}$ son las raíces de la ec.(\ref{eq:ecuacion_04_02_14}) y $m_{1} - m_{2} \not \in \mathbb{Z}$ entonces el método de Frobenius
% \begin{equation}
% y(x) = x^{m} \: \sum_{k=0}^{\infty} c_{k} \: x^{k}
% \label{eq:ecuacion_04_02_15}
% \end{equation}
% genera dos soluciones linealmente independientes para la ec. (\ref{eq:ecuacion_04_02_04}):
% \begin{equation}
% y_{1}(x) = x^{m_{1}} \sum_{k=0}^{\infty} a_{k} \: x^{k} \hspace{1cm} y_{2}(x) = x^{m_{2}} \sum_{k=0}^{\infty} b_{k} \: x^{k}
% \label{eq:ecuacion_04_02_16}
% \end{equation}
% \subsubsection{Raíces distintas con diferencia entera.}
% Si $m_{1}$, $m_{2}$ son las raíces de la ec.(\ref{eq:ecuacion_04_02_14}) y si $m_{1} - m_{2} \in \mathbb{Z}$, suponiendo que $m_{1} > m_{2}$. Definimos
% \begin{equation}
% N \equiv m_{1} - m_{2}
% \label{eq:ecuacion_04_02_37}
% \end{equation}
% \begin{enumerate}
% \item Si después de expandir la ec. (\ref{eq:ecuacion_04_02_08}) en series de potencias se llega a que el coeficiente que multiplica a $x^{m_{2}+N}$ es automáticamente cero, entonces usando la raíz más pequeña se pueden hallar dos soluciones en series de Frobenius.
% \item Si después de expandir (\ref{eq:ecuacion_04_02_08}) en series de potencias se llega a que el coeficiente que multiplica a $x^{m_{2}+N}$ no es automáticamente cero, entonces usando la raíz más grande hay una solución en serie de la forma
% \begin{equation}
% y_{1}(x) = x^{m_{1}} \sum_{k=0}^{\infty} a_{k} \: x^{k}
% \label{eq:ecuacion_04_02_38}
% \end{equation}
% y la segunda solución es de la forma
% \begin{equation}
% y_{2}(x) = - b_{N} \: y_{1}(x) \: \ln x + x^{m_{2}} \sum_{k=0}^{\infty} b_{k} \: x^{k}
% \label{eq:ecuacion_04_02_39}
% \end{equation}
% \end{enumerate}
% \subsubsection{Raíces repetidas.}
% Si $m_{1} = m_{2}$ en la ec. (\ref{eq:ecuacion_04_02_14}), las soluciones son de la forma:
% \begin{align}
% \begin{aligned}
% y_{1}(x) &= x^{m_{1}} \sum_{k=0}^{\infty} a_{k} \: x^{k} \\
% y_{2}(x) &= y_{1}(x) \: \ln x + x^{m_{1}} \sum_{k=0}^{\infty} b_{k} \: x^{k}
% \end{aligned}
% \label{eq:ecuacion_04_02_76}
% \end{align}
% \subsubsection{Ejercicios.}
% Encuentra la solución general de las siguientes EDO2H mediante el método de Frobenius:
% \begin{enumerate}
% \item $2 \: x \: y^{\prime \prime} + (1 + x) \: y^{\prime} + y = 0 $
% \item $x^{2} \: y^{\prime \prime} + x \: y^{\prime} + (x^{2} - \frac{1}{4}) \: y = 0 $
% \item $x^{2} \: y^{\prime \prime} + x \: y^{\prime} + x^{2} \: y = 0$
% \end{enumerate}
\section*{Expansión cerca de $x_{0}$}
La ecuación (\ref{eq:ecuacion_09_85}) es una expansión cerca del origen $x_{0} = 0$. Podemos re-emplazar la ec. (\ref{eq:ecuacion_09_85}) con
\begin{equation}
y(x) = \sum_{\lambda=0}^{\infty} a_{\lambda} \: (x - x_{0})^{k + \lambda}, \hspace{1cm} a_{0} \neq 0
\label{eq:ecuacion_09_95}
\end{equation}
De hecho, para las ecuaciones de Legendre, Chebyshev, y las hipergeométrica, la elección $x_{0} = 1$ tiene algunas ventajas. El punto $x_{0}$ no debe elegirse en una singularidad esencial, ya que el método de Frobenius probablemente fallará. La serie resultante ($x_{0}$ es un punto ordinario o punto singular regular) será válida donde converge. Podemos esperar una divergencia de algún tipo cuando $\abs{x - x_{0}} = \abs{z_{s} - x_{0}}$, donde $z_{s}$ es la singularidad más cercana a $x_{0}$ (en el plano complejo).

\vspace{0.3cm}
{\fontsize{12}{12}\selectfont
\renewcommand{\arraystretch}{2.0}
\begin{tabular}{p{8.2cm} >{\centering\arraybackslash}p{3cm} >{\centering\arraybackslash}p{2.5cm}}
\makecell{Ecuación} & \makecell{Singularidad \\ regular} & \makecell{Singularidad \\ irregular}\\ \hline
\makecell[l]{Hipergeométrica \\ $x(x - 1) \: y^{\prime \prime} + [(1 + a + b) \: x - c ] \: y^{\prime} + a \: b \: y = 0$} & $x = 0, 1, \infty$ & $--$ \\
\makecell[l]{Legendre \\ $(1 - x^{2}) \: y^{\prime \prime} - 2 \: x \: y^{\prime} +  l (l + 1) = 0$} &  $x = -1, 1, \infty$ & $--$ \\
\makecell[l]{Chebychev \\ $(1 - x^{2}) - x \: y^{\prime} + n^{2} \: y = 0$} & $x= -1, 1, \infty$ & $--$\\
\makecell[l]{Hipergeométrica confluente \\ $x \: y^{\prime \prime} + (c - x) \: y^{\prime} - a \: y =0 $} & 0 & $\infty$ \\
\makecell[l]{Bessel \\ $x^{2} \: y^{\prime \prime} + x \: y^{\prime} + (x^{2} - n^{2}) \: y = 0$} & 0 & $\infty$ \\
\makecell[l]{Laguerre \\ $x \: y^{\prime \prime} + (1 - x) \: y^{\prime} + a \: y = 0$} & 0 & $\infty$ \\
\makecell[l]{Oscilador armónico simple \\ $y^{\prime \prime} + \omega^{2} \: y = 0$} & $--$ & $\infty$ \\
\makecell[l]{Hermite \\ $ y^{\prime \prime} - 2 \: x \: y^{\prime} + 2 \: \alpha \: y = 0$} & $--$ & $\infty$
\end{tabular}
}
\section*{Simetría de las soluciones.}
Tengamos en cuenta que se obtuvo una solución de simetría par $y_{1}(x) = y_{1} (-x)$, y una solución de simetría impar, $y_{2}(x) = - y_{2}(-x)$. Esto no es sólo una casualidad, sino una consecuencia directa de la forma de la EDO. Escribiendo en general una EDO como
\begin{align}
\mathcal{L}(x) y(x) = 0
\label{eq:ecuacion_09_096}
\end{align}
en donde $\mathcal{L}$ es el operador diferencial, vemos que la ecuación del oscilador lineal (ec. \ref{eq:ecuacion_09_84}) es par, es decir
\begin{align}
\mathcal{L}(x) = \mathcal{L}(-x)
\label{eq:ecuacion_09_97}
\end{align}
Sin importar el tipo de operador diferencial tiene una paridad específica o simetría, ya sea par o impar, podemos intercambiar $-x$ y $-x$ en la ecuación (\ref{eq:ecuacion_09_096}) y se obtiene
\begin{align}
\pm \mathcal{L} y(-x) = 0
\label{eq:ecuacion_09_98}
\end{align}
en que $+$ es cuando $\mathcal{L}(x)$ es par, y $-$ cuando $\mathcal{L}$ es impar. Queda claro que si $y(x)$ es una solución de la ecuación diferencial, $y(-x)$ es también una solución. Entonces cualquier solución se resuelve en partes pares e impares
\begin{align}
y(x) = \dfrac{1}{2} \left[ y(x) + y(-x) \right] + \dfrac{1}{2} \left[ y(x) - y (-x) \right]
\label{eq:ecuacion_09_99}
\end{align}
el primer corchete corresponde a la solución para una solución par, mientras que el segundo proporciona una solución impar.
\section*{Limitaciones en el alcance de las series. Caso de la ecuación de Bessel.}
Abordar la ecuación del oscilador lineal fue relativamente muy sencillo: sustituimos la serie de potencias (ec. \ref{eq:ecuacion_09_85}) en la ecuación diferencial (ec. \ref{eq:ecuacion_09_84}), y obtuvimos dos soluciones independientes y no hubo mucho problema.
\par
Veamos lo que sucede si intentamos resolver la ecuación de Bessel
\begin{align}
x^{2} \, y^{\prime \prime} + x \, y^{\prime} + (x^{2} - n^{2}) \, y = 0
\label{eq:ecuacion_09_100}
\end{align}
dejando la notación $\displaystyle y^{\prime} = \dv{y}{x}$ y $\displaystyle y^{\prime \prime} = \dv[2]{y}{x}$. Entonces, suponemos que la solución es de la forma
\begin{align*}
y(x) = \sum_{\lambda=0}^{\infty} a_{\lambda} \; x^{k + \lambda}
\end{align*}
diferenciamos y sustituimos en la ecuación (\ref{eq:ecuacion_09_100}), el resultado es
\begin{align}
\begin{aligned}
\sum_{\lambda=0}^{\infty} a_{\lambda} & \, (k + \lambda)(k + \lambda - 1) \, x^{k + \lambda} + \sum_{\lambda=0}^{\infty} a_{\lambda} \, (k + \lambda) \, x^{k + \lambda} + \\[0.5em]
& + \sum_{\lambda=0}^{\infty} a_{\lambda} \, x^{k + \lambda + 2} - \sum_{\lambda=0}^{\infty} a_{\lambda} \, n^{2} \, x^{k + \lambda}  = 0
\end{aligned}
\label{eq:ecuacion_09_101}
\end{align}
Hacemos $\lambda = 0$ para obtener el coeficiente de $x^{k}$, la potencia más baja de $x$ que aparece del lado izquierdo es
\begin{align}
a_{0} \, [ k (k - 1) + k - n^{2} ] = 0
\label{eq:ecuacion_09_102}
\end{align}
y de nuevo $a_{0} = \neq 0$ por definición. La ecuación (\ref{eq:ecuacion_09_102}) nos conduce por tanto a la ecuación indicial
\begin{align}
k^{2} - n^{2} = 0
\label{eq:ecuacion_09_103}
\end{align}
con soluciones $k = \pm n$.
Veamos con interés el coeficiente para $x^{k+1}$. Obtenemos entonces
\begin{align*}
a_{1} \, [(k + 1) k + k + 1 - n^{2} ] = 0
\end{align*}
o equivalentemente
\begin{align}
a_{1} (k + 1 - n)(k + 1 + n) = 0
\label{eq:ecuacion_09_104}
\end{align}
Para $k = \pm n$ tanto $k + 1 -n$ o $k + 1 + n$ no se anulan, y \emph{debemos hacer} que $a_{1} = 0$.
\par
Continuando con el coeficiente de $x^{k+j}$ para $k=n$, hacemos $\lambda=j$ en el primero, segundo y cuarto término de la ec. (\ref{eq:ecuacion_09_101}), y hacemos que $\lambda = j - 2$ en el tercer término. Como se necesita que el coeficiente resultante de $x^{k+1}$ se anule, tenemos
\begin{align*}
a_{j} \, [(n + j)(n + j - 1) + (n + j) - n^{2}] + a_{j-2} = 0
\end{align*}
cuando $j$ se re-emplaza por $j+2$, podemos re-escribir para $j \geq 0$ como
\begin{align}
a_{j+2} = - a_{j} \dfrac{1}{(j + 2) \: (2 \: n + j + 2)}
\label{eq:ecuacion_09_105}
\end{align}
que sería la deseada relación de recurrencia. Repitiendo la relación, tendremos
\begin{align*}
a_{2} &= - a_{0} \dfrac{1}{2(2 \: n + 2)} = - \dfrac{a_{0} \: n!}{2^{2} \: 1! \: (n + 1)!} \\[0.5em]
a_{4} &= - a_{2} \dfrac{1}{4(2 \: n + 4)} =  \dfrac{a_{0} \: n!}{2^{4} \: 2! \: (n + 2)!} \\[0.5em]
a_{6} &= - a_{4} \dfrac{1}{6(2 \: n + 6)} =  \dfrac{a_{0} \: n!}{2^{6} \: 3! \: (n + 3)!} \hspace{1cm}\mbox{y así sucesivamente}
\end{align*}
Y en general
\begin{align}
a_{2 p} = (- 1)^{p} \; \dfrac{a_{0} \: n!}{2^{2p} \: p! \: (n+p)!}
\label{eq:ecuacion_09_106}
\end{align}
Acomodando los coeficientes en la solución en series, tenemos
\begin{align}
y(x) = a_{0} \: x^{n} \: \left[ 1 - \dfrac{n! \: x^{2}}{2^{2} \: 1! \: (n + 1)!} + \dfrac{n! \: x^{4}}{2^{4} \: 2! \: (n+2)!} - \ldots \right]
\label{eq:ecuacion_09_107}
\end{align}
En forma de suma:
\begin{align}
\begin{aligned} 
y(x) &= a_{0}\: \sum_{j=0}^{\infty} (-1)^{j} \: \dfrac{n! \:  x^{n + 2j}}{2^{2j} \: j! \: (n+j)!} = \\
&= a_{0} \: 2^{2} \: n! \: \sum_{j=0}^{\infty} (-1)^{j} \: \dfrac{1}{j! \: (n+j)!} \left( \dfrac{x}{2} \right)^{n + 2j}
\end{aligned}	
\label{eq:ecuacion_09_108}  
\end{align}
Que es la definición de la función de Bessel $J_{n}(x)$.
\par
 Cuando $k = -n$ y $n$ es no entero, podemos generar una segunda solución distinta, que se identifica como $J_{-n}(x)$. Pero cuando $-n$ es un entero negativo, se presentan algunos problemas. La relación de recurrencia para los coeficientes $a_{j}$ está dada por la ecuación (\ref{eq:ecuacion_09_105}), pero cambiando $2n$ por $-2n$, entonces, cuando $j + 2 = 2 n$ o $j = 2 (n - 1)$, el coeficiente $a_{j+2}$ se cancela y no se genera la solución en series. Esto lo podemos resolver (y se verá más adelante) por
\begin{align}
J_{-n} (x) = (-1)^{n} \: J_{n}(x) \hspace{1.5cm} n \text{ es un número entero}
\label{eq:ecuacion_09_109}
\end{align}
Esta segunda solución simplemente reproduce la primera. No se logró construir una segunda solución independiente para la ecuación de Bessel usando esta técnica, con $n$ entero.
\par
Mediante la sustitución en una serie infinita, se han obtenido dos soluciones para la ecuación del oscilador lineal y una para la ecuación de Bessel (dos cuando $n$ es un número entero). Cuando planteamos las preguntas:
\begin{itemize}
\item ¿Podemos hacer siempre esto?
\item ¿Es satisfactorio siempre este método?
\end{itemize}
La respuesta es \emph{no}, no siempre podemos hacer esto. El método de la solución en serie no siempre es el adecuado.
\subsection*{Singularidades regulares e irregulares.}
El éxito del método de la sustitución en serie depende de las raíces de la ecuación indicial y el grado de singularidad de los coeficientes en la EDO. Para comprender mejor el efecto de los coeficientes de la ecuación en este procedimiento de sustitución de serie simple, consideremos cuatro ecuaciones diferenciales simples:
\begin{subequations}
\begin{align}
y^{\prime \prime} - \dfrac{6}{x^{2}} \, y &= 0 \label{eq:ecuacion_09_110a} \\[0.25em]
y^{\prime \prime} - \dfrac{6}{x^{3}} \, y &= 0 \label{eq:ecuacion_09_110b} \\[0.25em]
y^{\prime \prime} + \dfrac{1}{x} \: y^{\prime} - \dfrac{a^{2}}{x^{2}} \, y &= 0 \label{eq:ecuacion_09_110c} \\[0.25em]
y^{\prime \prime} + \dfrac{1}{x^{2}} \: y^{\prime} - \dfrac{a^{2}}{x^{2}} \, y &= 0 \label{eq:ecuacion_09_110d}
\end{align}
\end{subequations}
Se puede demostrar fácilmente que para la ec. (\ref{eq:ecuacion_09_110a}), la ecuación de índices es
\begin{align*}
k^{2} - k - 6 = 0
\end{align*}
considerando que $k = 3, -2$. Ya que la ecuación es homogénea en $x$ (contando $\dv*[2]{x}$ como $x^{-2}$), no existe relación de recurrencia; $a_{i} = 0$ para $i > 0$. Sin embargo, se han logrado dos soluciones perfectamente satisfactorias: $x^{3}$ y $x^{-2}$.
% \subsection{Teorema de Fuchs.}
% La respuesta a la pregunta básica cuando se espera que el método de sustitución en series funcione, está dado por el teorema de Fuchs, que afirma que siempre podemos obtener por lo menos una solución en serie de potencias, siempre que se esté expandiendo alrededor de un punto que es un punto ordinario o en el peor de los casos, en un punto singular regular.
% \\
% A modo de resumen:
% \\
% Si se hace una expansión sobre un punto ordinario o en el peor de los casos, en una singularidad regular, la sustitución por una serie de potencias, devolverá al menos una solución (Teorema de Fuchs).
% \\
% Si obtenemos una o dos diferentes soluciones depende  de las raíces de la ecuación indicial:
% \begin{enumerate}
% \item si las dos raíces de la ecuación indicial son iguales, se obtiene una única solución con el método de sustitución con una serie de potencias.
% \item Si las dos raíces difieren por un número no entero, se pueden obtener dos soluciones independientes.
% \item Si las dos raíces difieren por un número entero, se toma la raíz de mayor valor para generar la solución. El raíz con valor menor puede o no generar una solución, ya que depende del comportamiento de los coeficientes.
% \end{enumerate}
% \section*{Singularidades en el infinito.}
% Considerando la ecuación
% \begin{equation}
% P(x) y^{\prime \prime} +  Q(x) y^{\prime} + R(x) y = 0
% \label{eq:ecuacion_5_4_01}
% \end{equation}
% en la vecindad de un punto singular $x_{0}$. Donde las funciones $P$, $Q$, y $R$ son polinomios que no tienen factores comunes, los puntos singulares de la ecuación (\ref{eq:ecuacion_5_4_01}) son los puntos para los que $P(x) = 0$.
% \\
% Para resolver la ecuación anterior en la vecindad del punto singular $x_{0}$, es necesario restringirse a los casos en los que las singularidades de las funciones $Q/P$ y $R/P$ en $x = x_{0}$ no sean demasiado severas, es decir, lo que podrían llamarse como ''singularidades débiles''.
% \\
% Las condiciones que distinguen las singularidades débiles son
% \begin{equation}
% \lim_{x \to x_{0}} (x - x_{0}) \dfrac{Q(x)}{P(x)} \hspace{1cm} \mbox{es finito}
% \label{eq:ecuacion_5_4_06}
% \end{equation}
% y
% \begin{equation}
% \lim_{x \to x_{0}} (x - x_{0})^{2} \dfrac{R(x)}{P(x)} \hspace{1cm} \mbox{es finito}
% \label{eq:ecuacion_5_4_07}
% \end{equation}
% que significa que la singularidad en $Q/P$ no puede ser peor que $(x - x_{0})^{-1}$ y que la singularidad en $R/P$ no puede ser peor que $(x - x_{0})^{-2}$.
% \\
% Para funciones más generales que los polinomios, $x_{0}$ es punto singular regular de la ecuación (\ref{eq:ecuacion_5_4_01}) si es punto singular y si tanto
% \begin{equation}
% (x - x_{0}) \dfrac{Q(x)}{P(x)} \hspace{1cm} \mbox{como} \hspace{1cm} (x - x_{0})^{2} \dfrac{R(x)}{P(x)}
% \label{eq:ecuacion_5_4_08}
% \end{equation}
% tienen series de Taylor convergentes alrededor de $x_{0}$, es decir, si las funciones de la ecuación (\ref{eq:ecuacion_5_4_08}) son analíticas en $x = x_{0}$.
% \subsection{Singularidades en el infinito.}
% Las definiciones de punto ordinario y de punto singular regular son válidas sólo si el punto $x_{0}$ es finito. A menudo es necesario analizar el punto en el infinito.
% \\
% Esto se logra al efectuar un cambio de variable $\xi = \frac{1}{x}$ y estudiar la ecuación resultante en $\xi = 0$.
% \\
% Se puede demostrar que para la ecuación
% \[ P(x) y^{\prime \prime} +  Q(x) y^{\prime} + R(x) y = 0 \]
% el punto en el infinito es un punto ordinario si
% \[ \dfrac{1}{P(1/\xi)} \left[ \dfrac{2 P(1/\xi)}{\xi} - \dfrac{Q(1/\xi)}{\xi^{2}} \right] \hspace{1cm} \mbox{y} \hspace{1cm} \dfrac{R(1/\xi)}{\xi^{4} P(1/\xi)}\]
% tienen desarrollor en serie de Taylor alrededor de $\xi = 0$. También se puede demostrar que el punto en el infinito es un punto singular regular si por lo menos una de las funciones anteriores no tiene un desarrollo en serie de Taylor, pero que tanto
% \[ \dfrac{\xi}{P(1/\xi)} \left[ \dfrac{2 P(1/\xi)}{\xi} - \dfrac{Q(1/\xi)}{\xi^{2}} \right] \hspace{1cm} \mbox{como} \hspace{1cm} \dfrac{R(1/\xi)}{\xi^{2} P(1/\xi)}\]
% sí tienen esos desarrollos.

\end{document}