\input{../Preambulos/preambulo_materiales}
\title{Segunda solución linealmente independiente \\ \large {Tema 2 - Matemáticas Avanzadas de la Física}\vspace{-3ex}}

\author{M. en C. Gustavo Contreras Mayén}
\date{ }

\pagestyle{fancy}
\fancyhf{}
\rhead{Curso MAF}
\lhead{\leftmark}
\rfoot{\thepage}
\setlength{\headheight}{16pt}%


\begin{document}
\maketitle
\fontsize{14}{14}\selectfont
%Referencia Arfken - 9.6 A Second Solution

\section{Una segunda solución.}

Con el método de Frobenius se obtuvo una solución de una EDO2H a partir de la sustitución en una serie de potencias. Esto es posible por el teorema de Fuchs, siempre que la serie de potencias se haga alrededor de un punto ordinario o un punto singular no esencial.
\par
No hay garantía de que este enfoque devuelva dos soluciones independientes que esperamos de una EDO lineal de segundo orden. Vamos a demostrar que una EDO de este tipo tiene como máximo dos soluciones linealmente independientes. De hecho, la técnica dio sólo una solución para la ecuación de Bessel (con $n$ un número entero).
\par
En esta clase desarrollamos dos métodos para obtener una segunda solución independiente:
\begin{enumerate}
\item Un método integral.
\item Una serie de potencias que contiene un término logarítmico.
\end{enumerate}
En primer lugar, sin embargo, consideramos la independencia de un conjunto de funciones.

\section{Independencia lineal de las soluciones.}

Dado un conjunto de funciones $\varphi_{\lambda}$, el criterio de dependencia lineal es la existencia de una relación de la forma
\begin{align}
\nsum_{\lambda} k_{\lambda} \: \varphi_{\lambda} = 0 
\label{eq:ecuacion_09_111}
\end{align}
en la cual, no necesariamente todos los coeficientes de $k_{\lambda}$ son cero. Dicho de otra forma, si la única solución de la ecuación (\ref{eq:ecuacion_09_111}) es $k_{\lambda} = 0$, para toda $\lambda$, el conjunto de funciones $\varphi_{\lambda}$ se dice que es \emph{linealmente independiente}.
\par
Pensemos en la dependencia lineal entre vectores. Consideremos $\vb{A}$, $\vb{B}$, y $\vb{C}$, en el espacio 3D, con $\vb{A} \cdot \vb{B} \cp \vb{C} \neq 0$. Entonces la relación no trivial de la forma
\begin{align}
a \: \vb{A} + b \: \vb{B} + c \: \vb{C} = 0
\label{eq:ecuacion_09_112}
\end{align}
no existe. Por lo que vemos que $\vb{A}$, $\vb{B}$, y $\vb{C}$ son linealmente independientes.
\par
En el caso de que un cuarto vector $\vb{D}$, se puede expresar como una combinación lineal de $\vb{A}$, $\vb{B}$, y $\vb{C}$, podemos escribir una ecuación de la forma
\begin{align}
\vb{D} -  a \: \vb{A} - b \: \vb{B} - c \: \vb{C} = 0
\label{eq:ecuacion_09_113}
\end{align}
y los cuatro vectores no son linealmente independientes. Si un conjunto de vectores o funciones son mutuamente ortogonales, entonces sabemos que son linealmente independientes. La ortogonalidad implica independencia lineal.
\par
Suponemos que las funciones $\varphi_{\lambda}$ son diferenciables, por tanto, podemos diferenciar la expresión (\ref{eq:ecuacion_09_111}) repetidamente, donde obtenemos el conjunto de ecuaciones
\begin{align}
\nsum_{\lambda} k_{\lambda} \: \varphi^{\prime}_{\lambda} &= 0 \label{eq:ecuacion_09_114} \\[0.5em]
\nsum_{\lambda} k_{\lambda} \: \varphi^{\prime \prime}_{\lambda} &= 0 \label{eq:ecuacion_09_115} \\[0.5em]
&\vdots \nonumber
\end{align}
y así, sucesivamente. Lo que nos proporciona un conjunto de ecuaciones lineales homogéneas, en donde los coeficientes $k_{\lambda}$ con cantidades desconocidas. Sabemos que existe una solución $k_{\lambda} \neq 0$, sólo si el determinante de los coeficientes de las $k_{\lambda}$ se anulan, esto implica que:
\begin{align}
\mdet{
\varphi_{1} & \varphi_{2} & \ldots & \varphi_{n} \\[0.5em]
\varphi^{\prime}_{1} & \varphi^{\prime}_{2} & \ldots & \varphi^{\prime}_{n} \\[0.5em]
\ldots & \ldots & \ldots & \ldots \\[0.5em]
\varphi^{(n-1)}_{1} & \varphi^{(n-1)}_{2} & \ldots & \varphi^{(n-1)}_{n} \\
} = 0
\label{eq:ecuacion_09_116}
\end{align}
A este determinante se le llama \emph{Wronskiano}:
\begin{enumerate}
\item Si el Wronskiano no es cero, entonces la ecuación (\ref{eq:ecuacion_09_111}) no tiene solución más que $k_{\lambda}=0$. El conjunto de funciones es por tanto, independiente.
\item Si el Wronskiano se anula en ciertos valores aislados del argumento, esto no prueba necesariamente la dependencia lineal (a menos que el conjunto de funciones sea de dos funciones). De cualquier manera, si el Wronskiano es cero en un rango amplio de la variable, las funciones $\varphi_{\lambda}$ son linealmente independientes dentro de ese rango.
\end{enumerate}
\par
\textbf{Ejemplo: Independencia lineal} 
\\
Las soluciones de la ecuación del oscilador lineal que hemos visto son $\varphi_{1} = \sin \: \omega x$ y $\varphi_{2} = \cos \: \omega x$. Por lo que el Wronskiano resulta ser
\begin{align*}
\mqty|
\sin \omega x & \cos \omega x \\
\omega \: \cos \omega x & - \omega \: \sin \omega x
| = -\omega \neq 0
\end{align*}
Estas dos soluciones $\varphi_{1}$ y $\varphi_{2}$ son por tanto linealmente independientes. Para estas dos funciones, esto significa que una no es múltiplo de la otra, lo cual es cierto.
\par
Sabemos que
\begin{align*}
\sin \omega x = \pm (1 - \cos^{2} \: \omega x)^{1/2}
\end{align*}
pero ésta no es una relación lineal de la forma que muestra la ec. (\ref{eq:ecuacion_09_111}).
\par
\textbf{Ejemplo: Dependencia lineal:}
\\
Consideremos ahora las soluciones de la ecuación de difusión unidimensional, tales que $\varphi_{1} = e^{x}$ y $\varphi_{2} = e^{-x}$, y agreguemos $\varphi_{3} = \cosh x$ que también es una solución. 
\par
El Wronskiano es:
\begin{align*}
\mqty|
e^{x}  & e^{-x} & \cosh x \\
e^{x}  & -e^{-x} & \sinh x \\
e^{x}  & e^{-x} & \cosh x
| = 0
\end{align*}
El determinante se anula para todos los valores de $x$, ya que el primer y tercer renglón son iguales. Por tanto $e^{x}$, $e^{-x}$ y $\cosh x$ son linealmente dependientes. Tenemos pues una relación de la forma (\ref{eq:ecuacion_09_111}):
\begin{align*}
e^{x} + e^{-x} - 2 \: \cosh x = 0 \hspace{2cm} \text{con } k_{\lambda} \neq 0
\end{align*}

\section{Segunda solución.}

Regresando a la ecuación diferencial lineal de segundo orden y homogénea de la forma
\begin{align}
\sderivada{y} + P(x) \, \pderivada{y} + Q(x) \, y = 0
\label{eq:ecuacion_09_118}
\end{align}
sean $y_{1}$ y $y_{2}$ dos soluciones independientes. El Wronskiano por definición es:
\begin{align}
W = y_{1} \, \pderivada{y}_{2} - \pderivada{y} \, y_{2}
\label{eq:ecuacion_09_119}
\end{align}
diferenciando el Wronskiano, obtenemos
\begin{align*}
\pderivada{W} &= \pderivada{y}_{1} \, \pderivada{y}_{2} + y_{1} \, \sderivada{y}_{2} - \sderivada{y}_{1} \, y_{2} - \sderivada{y}_{1} \: \pderivada{y}_{2} \\
&= y_{1} \big[ - P(x) \, \pderivada{y}_{2} - Q(x) \, y_{2} \big] - y_{2} \big[ - P(x) \, \pderivada{y}_{1} - Q(x)  \, y_{1}] \\
&= - P(x) \, \big[ y_{1} \, \pderivada{y}_{2} - \pderivada{y}_{1} \, y_{2} \big]
\end{align*}
Donde la expresión entre paréntesis es el Wronskiano mismo, por tanto
\begin{align}
\pderivada{W} = - P(x) \, W
\label{eq:ecuacion_09_120}
\end{align}
En el caso especial, si $P(x) = 0$, entonces
\begin{align}
\sderivada{y} + Q(x) \, y = 0
\label{eq:ecuacion_09_121}
\end{align}
y el Wronskiano
\begin{align}
W = y_{1} \, \pderivada{y}_{2} - \pderivada{y}_{1} \, y_{2} = \mbox{ constante}
\label{eq:ecuacion_09_1202}
\end{align}
Ya que nuestra ecuación diferencial es homogénea, podemos multiplicar las soluciones $y_{1}$ e $y_{2}$ por cualesquiera constantes, para ajustar que el valor del Wronskiano sea uno (ó $-1$).
\par
El caso $P(x) = 0$ aparece más frecuentemente de lo esperado:
\begin{itemize}
\item El Laplaciano $\laplacian$ en coordenadas cartesianas no contiene la primera derivada.
\item La dependencia radial de $\laplacian (\dfrac{\psi}{r})$ en coordenadas esféricas polares carece de la primera derivada radial.
\end{itemize}
Cualquier EDO2 lineal puede transformarse en una ecuación de la forma (ec. \ref{eq:ecuacion_09_121}).
\par
Para el caso general, vamos a suponer que tenemos una solución para la ecuación (\ref{eq:ecuacion_09_118}) sustituyendo una serie (o adivinando). Desarrollamos una segunda solución independiente para la cual $W \neq 0$, reescribimos la ecuación (\ref{eq:ecuacion_09_120}) como
\begin{align*}
\dfrac{\dd{W}}{W} = - P(x) \dd{x_{1}}
\end{align*}
integramos con respecto a $x$, desde $a$ hasta $x$ de donde obtenemos
\begin{align*}
\ln \dfrac{W(x)}{W(a)} = - \int_{a}^{x} P(x_{1}) \dd{x_{1}}
\end{align*}
que es lo mismo
\begin{align}
\setlength{\fboxsep}{2\fboxsep}\boxed{W(x) = W(a) \, \exp \left[ - \int_{a}^{x} P(x_{1}) \dd{x_{1}} \right]}
\label{eq:ecuacion_09_123}
\end{align}
Pero
\begin{align}
W(x) = y_{1} \: \pderivada{y}_{2} - \pderivada{y}_{1} \, y_{2} = y_{1}^{2} \dv{x} \left( \dfrac{y_{2}}{y_{1}} \right)
\label{eq:ecuacion_09_124}
\end{align}
Combinando las ecuaciones (\ref{eq:ecuacion_09_123}) y (\ref{eq:ecuacion_09_124}), tenemos que
\begin{align}
\dv{x} \left( \dfrac{y_{2}}{y_{1}} \right) =  W(a) \, \dfrac{\exp \left[\displaystyle - \int_{a}^{x} \, P(x_{1}) \dd{x_{1}} \right]}{y^{2}_{1}}
\label{eq:ecuacion_09_125}
\end{align}
Finalmente integramos la ecuación (\ref{eq:ecuacion_09_125}) de $x_{2} = b$ a $x_{2} = x$, para obtener
\begin{align}
y_{2} = y_{1} \: W(a) \, \int_{b}^{x} \dfrac{\exp \left[ \displaystyle - \int_{a}^{x_{2}} P(x_{1}) \dd{x_{1}} \right]}{[y_{1}(x_{2})]^{2}} \dd{x_{2}}
\label{eq:ecuacion_09_126}
\end{align}
Donde $a$ y $b$ son constantes arbitrarias, los términos $y_{1}(x)y_{2}(b)/y_{1}(b)$ se han omitido, ya que no conducen a algo.
\par
Para $W(a)$, el Wronskiano evaluado en $x = a$, es una constante y las soluciones de la ecuación diferencial homogénea siempre contienen un factor de normalización desconocido, hacemos $W(a) = 1$ y escribimos
\begin{align}
\setlength{\fboxsep}{2\fboxsep}\boxed{y_{2}(x) =  y_{1} (x) \,  \int^{x} \dfrac{\exp \left[ \displaystyle - \int^{x_{2}} P(x_{1}) \dd{x_{1}} \right]}{[y_{1}(x_{2})]^{2}} \dd{x_{2}}}
\label{eq:ecuacion_09_127}
\end{align}
en donde se han omitido los límites inferiores de integración $x_{1} = a$ y $x_{2} = b$. En caso contrario, simplemente originan una contribución igual a una constante multiplicada por al primera solución conocida $y_{1}(x)$ lo cual consecuentemente no agrega nada nuevo.
\par
Si tenemos el caso especial e importante, cuando $P(x) = 0$, la ecuación (\ref{eq:ecuacion_09_127}), toma la forma:
\begin{align}
y_{2}(x) =  y_{1}(x) \, \int^{x} \dfrac{\dd{x_{2}}}{[y_{1}(x_{2})]^{2}}
\label{eq:ecuacion_09_128}
\end{align}
Lo que significa que usando ya sea (\ref{eq:ecuacion_09_127}) o (\ref{eq:ecuacion_09_128}), podemos tomar una solución conocida y luego integrando, podemos generar una segunda solución independiente.
\par
\begin{ejemplo}{Segunda solución del oscilador lineal}
De la ecuación
\begin{align*}
\dv[2]{y}{x} + y = 0 \hspace{1cm} \mbox{con } P(x) = 0
\end{align*}
hacemos una solución tipo $y_{1} = \sin x$, aplicando la solución (\ref{eq:ecuacion_09_128}), resulta
\begin{align*}
y_{2}(x) &= \sin x \int^{x} \dfrac{\dd{x_{2}}}{\sin^{2} x_{2}} \\
&= \sin x (-\cot x) = - \cos x
\end{align*}
La cual es obviamente independiente (y no un múltiplo lineal) de $\sin x$.
\end{ejemplo}

\section{Desarrollo en series de una segunda solución.}

Para una mayor comprensión en la naturaleza de la segunda solución de la ecuación diferencial, veamos los siguientes pasos:
\begin{enumerate}
\item Expresamos $P(x)$ y $Q(x)$ en la ecuación (\ref{eq:ecuacion_09_118}) como
\begin{align}
P(x) = \nsum_{i=-1}^{\infty} p_{i} \: x^{i} \hspace{2cm} Q(x) = \nsum_{j=-2}^{\infty} q_{j} \: x^{j}
\label{eq:ecuacion_09_129}
\end{align}
Los límites inferiores de las sumas se eligen para crear una potencial singularidad regular (en el origen). Estas condiciones satisfacen el Teorema de Fuchs.
\item Desarrollamos los primeros términos de la solución en series de potencias.
\item Usamos la solución obtenida como $y_{1}$, obtenemos una segunda solución en tipo de series $y_{2}$, con la ecuación (\ref{eq:ecuacion_09_127}) integramos término a término.
\end{enumerate}
Siguiendo el paso 1, tenemos
\begin{equation}
\sderivada{y} + (p_{-1} \, x^{-1} + p_{0} + p_{1} \, x + \ldots) \, \pderivada{y} + (q_{-2} \, x^{-2} + q_{-1} \, x^{-1} + \ldots) \, y = 0
\label{eq:ecuacion_09_130}
\end{equation}
en donde el punto $x = 0$ es en el peor de los casos un punto singular regular. Si $p_{-1} = q_{-1} = q_{-2} = 0$, se reduce a un punto ordinario. Sustituimos
\begin{align*}
y = \nsum_{\lambda = 0}^{\infty} a_{\lambda} \: x^{k + \lambda}
\end{align*}
Obtenemos (paso 2)
\begin{align}
\begin{aligned}
\nsum_{\lambda=0}^{\infty} (k &+ \lambda)(k + \lambda - 1) \: a_{\lambda} \, x^{k + \lambda - 2} + \nsum_{i=-1}^{\infty} p_{i} \, x^{i} \nsum_{\lambda=0}^{\infty} (k + \lambda) \,  a_{\lambda} x^{k + \lambda - 1} + \\
&+ \nsum_{j=-2}^{\infty} q_{j} \, x^{j} \: \nsum_{\lambda=0}^{\infty} a_{\lambda} \, x^{k + \lambda} = 0
\end{aligned}
\label{eq:ecuacion_09_131}
\end{align}
suponiendo que $p_{-1} \neq 0, q_{-2} \neq 0$, la ecuación de índices es:
\begin{align*}
k \, (k - 1) + p_{-1} \, k + q_{-2} = 0
\end{align*}
lo que hace que el coeficiente neto de $x^{k-2}$ sea igual a cero, por lo que se reduce a
\begin{align}
k^{2} + (p_{-1} - 1) \, k + q_{-2} = 0
\label{eq:ecuacion_09_132}
\end{align}
Escribimos las raíces de la ecuación de índices como $k = \alpha$ y $k= \alpha - n$ donde $n$ es cero o un entero positivo (si $n$ es no entero, esperamos dos soluciones independientes en series). Por lo que
\begin{align}
(k - \alpha)(k - \alpha + n) = 0
\label{eq:ecuacion_09_133}
\end{align}
que es lo mismo
\begin{align*}
k^{2} + (n - 2 \, \alpha) \, k + \alpha \, (\alpha - n) = 0
\end{align*}
Igualando los coeficientes de $k$ en las ecuaciones (\ref{eq:ecuacion_09_132}) y (\ref{eq:ecuacion_09_133}), tenemos
\begin{align}
p_{-1} -1 = n - 2 \: \alpha
\label{eq:ecuacion_09_134}
\end{align}
La solución en series conocida corresponde al valor más grande de la raíz $k = \alpha$, que se escribe como
\begin{align*}
y_{1} =  x^{\alpha} \, \nsum_{\lambda=0}^{\infty} a_{\lambda} \: x^{\lambda}
\end{align*}
Sustituimos la solución en series en la ecuación (\ref{eq:ecuacion_09_127}) - Paso 3- y vemos que
\begin{align}
y_{2}(x) = y_{1} (x) \, \int^{x} \dfrac{\exp \left[ \displaystyle - \int_{a}^{x_{2}} \nsum_{i=-1}^{\infty} p_{i} \, x^{i}_{1} \, \dd{x_{1}} \right] }{x_{2}^{2 \, \alpha} \left( \displaystyle \nsum_{\lambda=0}^\infty a_{\lambda} \, x_{2}^{\lambda} \right)^{2} } \dd{x_{2}}
\label{eq:ecuacion_09_135}
\end{align}
donde las soluciones $y_{1}$ y $y_{2}$ se han normalizado de modo que el Wronskiano $W(a) = 1$. 
\par
Mirando primero el argumento del factor exponencial
\begin{align}
\int_{a}^{x_{2}} \nsum_{i=-1}^{\infty} p_{i} \, x_{1}^{i} \dd{x_{1}} = p_{-1} \, \ln x_{2} + \nsum_{k=0}^{\infty} \dfrac{p_{k}}{k + 1} \, x_{2}^{k + 1} + f(a)
\label{eq:ecuacion_09_136}
\end{align}
donde $f(a)$ es una constante de integración que depende de $a$. De aquí resulta:
\begin{align}
\begin{aligned}
&{}\exp \left( - \int_{a}^{x_{2}} \nsum_{i} p_{i} \, x_{1}^{i} \dd{x_{1}} \right) =  \\
&= \exp [ - f(a) ] \, x_{2}^{-p_{-1}} \exp \left( - \nsum_{k=0}^{\infty} \dfrac{p_{k}}{k+1} \, x_{2}^{k+1} \right)  \\
&= \exp [ - f(a) ] \, x_{2}^{-p_{-1}} \left[ 1 - \nsum_{k=0}^{\infty} \dfrac{p_{k}}{k+1} \, x_{2}^{k+1} + \dfrac{1}{2!} \left( \nsum_{k=0}^{\infty} \dfrac{p_{k}}{k+1} \, x_{2}^{k+1} \right)^{2} + \ldots \right]
\end{aligned}
\label{eq:ecuacion_09_137}
\end{align}
Esta expansión en serie de la exponencial ciertamente converge, si la expansión original del coeficiente $P(x)$ converge uniformemente.
\par
El denominador en la ecuación (\ref{eq:ecuacion_09_135}) puede manejarse como
\begin{align}
\begin{aligned}[b]
\left[ x_{2}^{2 \, \alpha} \left( \nsum_{\lambda=0}^{\infty} a_{\lambda} \, x_{2}^{\lambda} \right)^{2} \right]^{-1} &= x_{2}^{-2 \, \alpha} \left( \nsum_{\lambda=0}^{\infty} a_{\lambda} \, x_{2}^{\lambda} \right)^{2}  \\
&= x_{2}^{-2 \, \alpha} \nsum_{\lambda=0}^{\infty} b_{\lambda} \, x_{2}^{\lambda}
\end{aligned}
\label{eq:ecuacion_09_138}
\end{align}
Haciendo a un lado los factores constantes considerando que podemos tener $W(a) = 1$, para obtener
\begin{align}
y_{2}(x) =  y_{1}(x) = \int^{x} x_{2}^{-p_{-1}-2 \alpha} \left( \nsum_{\lambda=0}^{\infty} c_{\lambda} \, x_{2}^{\lambda} \right) \dd{x_{2}} 
\label{eq:ecuacion_09_139}
\end{align}
de las raíces de la ecuación de índices (ec. \ref{eq:ecuacion_09_134})
\begin{align}
x_{2}^{-p_{-1} - 2 \, \alpha} = x_{2}^{-n-1}
\end{align}
si suponemos que $n$ es entero, al sustituir el resultado en ec. (\ref{eq:ecuacion_09_139}) obtenemos 
\begin{align}
y_{2}(x) = y_{1}(x) \int^{x} (c_{0} \, x_{2}^{-n-1} + c_{1} \, x_{2}^{-n} + c_{2} \, x_{2}^{-n+1} + \ldots + c_{n} \, x^{-1} + \ldots ) \dd{x_{2}}
\label{eq:ecuacion_09_141}
\end{align}
La integración anterior nos devuelve un coeficiente de $y_{1}(x)$ formado de dos partes
\begin{enumerate}
\item Una serie de potencias que inicia en $x^{-n}$
\item Un término logarítmico de la integración de $x^{-1}$ (cuando $\lambda=n$). Este término siempre aparecerá cuando $n$ sea un entero, a menos de que $c_{n}$ fortuitamente se anule.
\end{enumerate}
\begin{ejemplo}{Una seguna solución (linealmente independiente) para la ecuación de Bessel.}

De acuerdo con la ec. de Bessel 
\begin{align}
x^{2} \: y^{\prime \prime} + x \: y^{\prime} + (x^{2} - n^{2}) \: y = 0
\label{eq:ecuacion_09_100}
\end{align}
(dividida entre $x^{2}$ para concondar con la ec. (\ref{eq:ecuacion_09_118})), se tiene
\begin{align*}
P(x) = x^{-1} \hspace{1.5cm} Q(x) = 1 \hspace{1.5cm} \text{con } n = 0
\end{align*}
En consecuencia $p_{-1} = 1, q_{0} = 1$, todas las otras $p_{i}$ y $q_{j}$ se anulan- La ecuación de índices de Bessel es
\begin{align*}
k^{2} = 0
\end{align*}
que es la ecuación $k^{2} - n^{2} = 0$ con $n = 0$. En consecuencia, se verifican las ecs. (\ref{eq:ecuacion_09_132}) a la (\ref{eq:ecuacion_09_134}) con $n$ y $\alpha = 0$.
\par
Nuestra primera solución queda disponible de la ecuación
\begin{align}
y(x) = a_{0} \, 2^{n} \, n! \, \nsum_{j=0}^{\infty} (-1)^{j} \, \dfrac{1}{j! \, (n + j)!} \left( \dfrac{x}{2} \right)^{n +2j}
\label{eq:ecuacion_09_108}
\end{align}
Hacemos un etiquetado para concordar con la función de Bessel de orden $0$ (y haciendo $a_{0} = 1)$, obtenemos
\begin{subequations}
\begin{align}
y_{1} (x) = J_{0} (x) =  1 - \dfrac{x^{2}}{4} + \dfrac{x^{4}}{64} - \order{x^{6}}
\label{eq:ecuacion_09_142a}
\end{align}
Ahora, sustituyendo todo esto en la ec. (\ref{eq:ecuacion_09_127}), se tiene el caso específico a la ec. (\ref{eq:ecuacion_09_135}):
\begin{align}
y_{2} (x) = J_{0} (x) \; \int^{x} \dfrac{\exp(\displaystyle - \int^{x^{2}} x_{1}^{-1} \dd{x_{1}})}{\left[ 1 - \dfrac{x^{2}}{4} + \dfrac{x^{4}}{64} - \ldots\right]^{2}} \dd{x_{2}}
\label{eq:ecuacion_09_142b}
\end{align}
Del numerador del integrando
\begin{align*}
\exp \left[ - \int^{x^{2}} \dfrac{\dd{x_{1}}}{x_{1}} \right]= \exp (\ln x_{2}) = \dfrac{1}{x_{2}} 
\end{align*}
Esto corresponde a $x_{2}^{p-1}$ en la ec. (\ref{eq:ecuacion_09_137}). Del denominador del integrando, usando la expansión binomial, obtenemos
\begin{align*}
\left[ 1 - \dfrac{x^{2}}{4} + \dfrac{x^{4}}{64} - \ldots \right]^{-2} = 1 + \dfrac{x_{2}^{2}}{2} + \dfrac{5 \, x_{2}^{4}}{32} + \ldots
\end{align*}
Correspondiente a la ec. (\ref{eq:ecuacion_09_139}), se tiene
\begin{align}
y_{2} (x) &= J_{0} (x) \, \int^{x} \dfrac{1}{x_{2}} \, \left[ 1 + \dfrac{x_{2}^{2}}{2} + \dfrac{5 \, x_{2}^{4}}{32} + \ldots \right] \dd{x_{2}} \nonumber \\[0.5em]
&= J_{0} (x) \, \left\{ \ln x + \dfrac{x^{2}}{4} + \dfrac{5 \, x^{4}}{128} + \ldots  \right\}
\label{eq:ecuacion_09_142c}
\end{align}
Comprobemos este resultado. Como veremos en el Tema de Funciones Especiales, una solución a la ecuación de Bessel está dada por la función de Neumann de orden $0$
\begin{align}
\begin{aligned}
N_{0}(x) &= \dfrac{2}{\pi} (\ln x + \gamma - \ln 2) \,J_{0} (x) + \\
&+ \dfrac{2}{\pi} \left\{ \dfrac{x^{2}}{4} + \dfrac{3 \, x^{4}}{128} + \ldots   \right\}
\end{aligned}
\label{eq:ecuacion_09_142d}
\end{align}
Se originan dos conceptos:
\begin{enumerate}
\item Ya que la ecuación de Bessel es homogénea, podemos multiplicar $y_{2}$ por cualquier constante. Para igualar con $N_{0}(x)$ simplemente se multiplica $y_{2}(x)$ por $\dfrac{2}{\pi}$.
\item Para la segunda solución $\dfrac{2}{\pi} \, y_{2}$, se puede agregar cualquier constante múltiplo de la primera solución. Nuevamente, para igualar $N_{0}(x)$ se agrega
\begin{align*}
\dfrac{2}{\pi} \big[ - \ln 2 + \gamma \big] \, J_{0} (x)
\end{align*}
donde $\gamma$ es la constante de Euler-Mascheroni. Nuestra segunda solución modificada es:
\begin{align}
y_{2} (x) = \dfrac{2}{\pi} \big[ - \ln 2 + \gamma \big] \, J_{0} (x) + \dfrac{2}{\pi} \, J_{0} (x) \, \left\{ \dfrac{x^{2}}{4} + \dfrac{5 \, x^{4}}{128} + \ldots   \right\}
\label{eq:ecuacion_09_142e}
\end{align}
\end{enumerate}
Ahora, la comparación con $N_{0}(x)$ se transforma en una multiplicación de $J_{0}(x)$ de la ec. (\ref{eq:ecuacion_09_142a}) y el paréntesis curvado de la ec. (\ref{eq:ecuacion_09_142c}). La multiplicación se comprueba a través de los términos de orden $x^{2}$ y $x^{4}$, la cual hemos llevamos para todo. Nuestra segunda solución de las ecs. (\ref{eq:ecuacion_09_127}) y (\ref{eq:ecuacion_09_135}) concuerda con la segunda solución estándar, la función de Neumann de orden $0$, $N_{0}(x)$.
\par
De acuerdo con el análisis precedente, la segunda solución de la ec. (\ref{eq:ecuacion_09_118}), $y_{2}(x)$, se puede escribir de la forma
\begin{align}
y_{2}(x) = y_{1} (x) \, \ln x + \nsum_{j=-n}^{\infty} d_{j} \, x^{j+\alpha}
\label{eq:ecuacion_09_142f}
\end{align}
es decir, la primera solución multiplicada por $\ln x$ y otra serie de potencias, ésta última comenzando con $x^{\alpha - n}$, lo cual significa que podemos buscar un término logarítmico cuando la ecuación de índices proporciona tan sólo una solución en serie.
\par
Con la forma de la segunda solución dada por la ec. (\ref{eq:ecuacion_09_142f}), podemos sustituir la ec. (\ref{eq:ecuacion_09_142f}) en la ecuación diferencial original y determinar los coeficientes $d_{j}$
\end{subequations}
\end{ejemplo}
La segunda solución normalmente será divergente en el origen debido al factor logarítmico y las potencias negativas de la serie. Debido a esto, es que $y_{2}(x)$ se denomina frecuentemente \textbf{solución irregular}. La primera solución en serie $y_{1}(x)$, que normalmente converge en el origen, se denomina \textbf{solución regular}.
\par
\textbf{Resumen: } Las dos soluciones vistas en este apartado, proporciona una solución completa de la EDO2H lineal, suponiendo que el punto de desarrollo no es peor que una singularidad regular. Al menos se puede obtener una solución mediante la sustitución en series de potencias. Una segunda solución linealmente independiente se puede construir por medio de la doble integral con el Wronskiano. Esto es todo lo posible: \emph{no hay una tercera solución linealmente independiente}.
\par
La ED2 lineal y \textit{no homogénea} tendrá una solución adicional: la \emph{solución particular}. Esta solución particular se puede obtener por el método de variación de parámetros, o por medio de técnicas tales como las funciones de Green.

% \section{Ejercicios a cuenta.}
% %Ref. Arfken (2006) 9.6 Second Solution. Exercises

% \begin{enumerate}
% \item \textbf{Ejercicio a cuenta (24). } Usando el determinante Wronskiano, demuestra que el conjunto de funciones:
% \begin{align*}
% \left\{ 1, \dfrac{x^{n}}{n!} \hspace{0.3cm} (n = 1, 2, \ldots, N) \right\}    
% \end{align*}
% es linealmente independiente.
% \item \textbf{Ejercicio a cuenta (25). }Las tres funciones $\sin x$, $e^{x}$, $e^{-x}$ son linealmente independientes, es decir, ninguna de ellas puede escribirse como combinación lineal de las otras dos. Demuestra que el Wronskiano de $\sin x$, $e^{x}$ y $e^{-x}$ se anula solo en algunos puntos aislados.
% \item \textbf{Ejercicio a cuenta (26). }A partir de la ecuación diferencial de Hermite\footnote{Recuerda que ya será posible darle un nombre a ciertas expresiones, que posteriormente estudiaremos más a fondo, en este momento, llamarlas con su nombre nos dará la oportunidad de identificarlas.}:
% \begin{align*}
% \sderivada{y} - 2 \, x \, \pderivada{y} +  2 \, \alpha \, y = 0
% \end{align*}
% \begin{enumerate}
% \item Para $\alpha = 0$ es $y_{1} = 1$. Encuentra una segunda solución $y_{2}(x)$, usando la ec. (\ref{eq:ecuacion_09_127}).
% \item Encuentra una segunda solución para $\alpha = 1$, donde $y_{1}(x) = x$, usando la ec. (\ref{eq:ecuacion_09_127}).
% \end{enumerate}
% \item Una solución para la ecuación de Chebychev:
% \begin{align*}
% (1 - x^{2}) \, \sderivada{y} -  x \, \pderivada{y} + n^{2} \, y = 0
% \end{align*}
% para $n = 0$ es $y_{1} = 1$.
% Usando la ec. (\ref{eq:ecuacion_09_127}), desarrolla una segunda solución linealmente independiente.
% \end{enumerate}

\end{document}
